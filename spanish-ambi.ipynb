{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "888df6d4-a41a-444a-9622-243929a426cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fbdf79b8-cafb-496d-9273-df81750ae16a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-04 01:43:42.636767: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-05-04 01:43:42.684508: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-05-04 01:43:42.684960: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-04 01:43:43.503379: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#NLTK\n",
    "import nltk\n",
    "from nltk import word_tokenize, WordPunctTokenizer, regexp_tokenize\n",
    "from nltk import word_tokenize, WordPunctTokenizer, regexp_tokenize\n",
    "\n",
    "#Plotting \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "#Keras\n",
    "from numpy import array\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c1547fa-6c45-4526-8065-47a7bf209d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(category):\n",
    "    dict = {'18-22F' : 0.85556179, '23-45F' : 0.87431270, '46+F' : 0.83906224, '46+M' : 0.83220058, '23-45M' : 0.83609059, '18-22M' : 0.84184846}\n",
    "    return dict[category]\n",
    "\n",
    "def label(genders, ages, labels):\n",
    "    yes = 0\n",
    "    no = 0\n",
    "    for i in range(0, 6):\n",
    "        category = ages[i] + genders[i]\n",
    "        if labels[i] == 'YES':\n",
    "            yes += score(category)\n",
    "        else:\n",
    "            no += score(category)\n",
    "    if yes > no:\n",
    "        return 'YES'\n",
    "    else:\n",
    "        return 'NO'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "446588d0-3e43-42f9-8e90-dec7ff24a9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_yes_es = pd.read_json('training/EXIST2023_training.json')\n",
    "train_yes_es=train_yes_es.transpose()\n",
    "train_yes_es=train_yes_es.loc[train_yes_es['lang'].apply(lambda x:x=='es')]\n",
    "train_yes_es=train_yes_es.loc[train_yes_es['labels_task1'].apply(lambda x:x.count('YES')>3)]\n",
    "train_yes_es['label1']='YES'\n",
    "\n",
    "#NO\n",
    "\n",
    "train_no_es = pd.read_json('training/EXIST2023_training.json')\n",
    "train_no_es=train_no_es.transpose()\n",
    "train_no_es=train_no_es.loc[train_no_es['lang'].apply(lambda x:x=='es')]\n",
    "train_no_es=train_no_es.loc[train_no_es['labels_task1'].apply(lambda x:x.count('NO')>3)]\n",
    "train_no_es['label1']='NO'\n",
    "\n",
    "\n",
    "train_amb = pd.read_json('training/EXIST2023_training.json')\n",
    "train_amb=train_amb.transpose()\n",
    "train_amb=train_amb.loc[train_amb['lang'].apply(lambda x:x=='es')]\n",
    "train_amb=train_amb.loc[train_amb['labels_task1'].apply(lambda x:x.count('YES')==3)]\n",
    "for i, sample in enumerate(train_amb.itertuples()):\n",
    "    truelabel = label(sample[6], sample[7], sample[8])\n",
    "    train_amb.at[sample[0], 'label1'] = truelabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d63b61f-a4da-4d3a-9fb5-14df26d92e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_yes_es = pd.read_json('dev/EXIST2023_dev.json')\n",
    "validation_yes_es=validation_yes_es.transpose()\n",
    "validation_yes_es=validation_yes_es.loc[validation_yes_es['lang'].apply(lambda x:x=='es')]\n",
    "validation_yes_es=validation_yes_es.loc[validation_yes_es['labels_task1'].apply(lambda x:x.count('YES')>3)]\n",
    "validation_yes_es['label1']='YES'\n",
    "\n",
    "#NO\n",
    "\n",
    "validation_no_es = pd.read_json('dev/EXIST2023_dev.json')\n",
    "validation_no_es=validation_no_es.transpose()\n",
    "validation_no_es=validation_no_es.loc[validation_no_es['lang'].apply(lambda x:x=='es')]\n",
    "validation_no_es=validation_no_es.loc[validation_no_es['labels_task1'].apply(lambda x:x.count('NO')>3)]\n",
    "validation_no_es['label1']='NO'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5f160df-2978-45b4-aa39-477fc8ee244f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_EXIST</th>\n",
       "      <th>lang</th>\n",
       "      <th>tweet</th>\n",
       "      <th>number_annotators</th>\n",
       "      <th>annotators</th>\n",
       "      <th>gender_annotators</th>\n",
       "      <th>age_annotators</th>\n",
       "      <th>labels_task1</th>\n",
       "      <th>labels_task2</th>\n",
       "      <th>labels_task3</th>\n",
       "      <th>split</th>\n",
       "      <th>label1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>300002</td>\n",
       "      <td>es</td>\n",
       "      <td>@anacaotica88 @MordorLivin No me acuerdo de lo...</td>\n",
       "      <td>6</td>\n",
       "      <td>[Annotator_731, Annotator_732, Annotator_315, ...</td>\n",
       "      <td>[F, F, F, M, M, M]</td>\n",
       "      <td>[18-22, 23-45, 46+, 18-22, 23-45, 46+]</td>\n",
       "      <td>[YES, YES, NO, YES, YES, YES]</td>\n",
       "      <td>[JUDGEMENTAL, REPORTED, -, JUDGEMENTAL, JUDGEM...</td>\n",
       "      <td>[[IDEOLOGICAL-INEQUALITY, STEREOTYPING-DOMINAN...</td>\n",
       "      <td>DEV_ES</td>\n",
       "      <td>YES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>300004</td>\n",
       "      <td>es</td>\n",
       "      <td>Also mientras les decia eso la señalaba y deci...</td>\n",
       "      <td>6</td>\n",
       "      <td>[Annotator_259, Annotator_739, Annotator_291, ...</td>\n",
       "      <td>[F, F, F, M, M, M]</td>\n",
       "      <td>[18-22, 23-45, 46+, 18-22, 23-45, 46+]</td>\n",
       "      <td>[NO, YES, YES, YES, YES, YES]</td>\n",
       "      <td>[-, REPORTED, REPORTED, REPORTED, JUDGEMENTAL,...</td>\n",
       "      <td>[[-], [SEXUAL-VIOLENCE], [SEXUAL-VIOLENCE], [S...</td>\n",
       "      <td>DEV_ES</td>\n",
       "      <td>YES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>300007</td>\n",
       "      <td>es</td>\n",
       "      <td>@DavidGR18 @pppbernat @abc_es @agarzon @IreneM...</td>\n",
       "      <td>6</td>\n",
       "      <td>[Annotator_731, Annotator_732, Annotator_315, ...</td>\n",
       "      <td>[F, F, F, M, M, M]</td>\n",
       "      <td>[18-22, 23-45, 46+, 18-22, 23-45, 46+]</td>\n",
       "      <td>[YES, YES, NO, YES, YES, YES]</td>\n",
       "      <td>[DIRECT, DIRECT, -, DIRECT, JUDGEMENTAL, REPOR...</td>\n",
       "      <td>[[IDEOLOGICAL-INEQUALITY, SEXUAL-VIOLENCE, MIS...</td>\n",
       "      <td>DEV_ES</td>\n",
       "      <td>YES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>300008</td>\n",
       "      <td>es</td>\n",
       "      <td>@DavidArranzVox @AnabelAlonso_of Uyyy a q huel...</td>\n",
       "      <td>6</td>\n",
       "      <td>[Annotator_742, Annotator_743, Annotator_195, ...</td>\n",
       "      <td>[F, F, F, M, M, M]</td>\n",
       "      <td>[18-22, 23-45, 46+, 18-22, 23-45, 46+]</td>\n",
       "      <td>[YES, YES, NO, YES, YES, YES]</td>\n",
       "      <td>[JUDGEMENTAL, JUDGEMENTAL, -, REPORTED, JUDGEM...</td>\n",
       "      <td>[[IDEOLOGICAL-INEQUALITY, STEREOTYPING-DOMINAN...</td>\n",
       "      <td>DEV_ES</td>\n",
       "      <td>YES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>300010</td>\n",
       "      <td>es</td>\n",
       "      <td>@kokreto84 @Play87834898 @venusoncrack Me gust...</td>\n",
       "      <td>6</td>\n",
       "      <td>[Annotator_744, Annotator_745, Annotator_746, ...</td>\n",
       "      <td>[F, F, F, M, M, M]</td>\n",
       "      <td>[18-22, 23-45, 46+, 18-22, 23-45, 46+]</td>\n",
       "      <td>[YES, YES, YES, YES, YES, YES]</td>\n",
       "      <td>[DIRECT, REPORTED, DIRECT, JUDGEMENTAL, DIRECT...</td>\n",
       "      <td>[[MISOGYNY-NON-SEXUAL-VIOLENCE], [STEREOTYPING...</td>\n",
       "      <td>DEV_ES</td>\n",
       "      <td>YES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>485</th>\n",
       "      <td>300534</td>\n",
       "      <td>es</td>\n",
       "      <td>El Gobernador @samuel_garcias  se da el lujo d...</td>\n",
       "      <td>6</td>\n",
       "      <td>[Annotator_726, Annotator_727, Annotator_357, ...</td>\n",
       "      <td>[F, F, F, M, M, M]</td>\n",
       "      <td>[18-22, 23-45, 46+, 18-22, 23-45, 46+]</td>\n",
       "      <td>[NO, YES, YES, NO, NO, NO]</td>\n",
       "      <td>[-, REPORTED, JUDGEMENTAL, -, -, -]</td>\n",
       "      <td>[[-], [IDEOLOGICAL-INEQUALITY], [STEREOTYPING-...</td>\n",
       "      <td>DEV_ES</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>486</th>\n",
       "      <td>300538</td>\n",
       "      <td>es</td>\n",
       "      <td>Violencia simbólica también es un Presidente q...</td>\n",
       "      <td>6</td>\n",
       "      <td>[Annotator_750, Annotator_751, Annotator_189, ...</td>\n",
       "      <td>[F, F, F, M, M, M]</td>\n",
       "      <td>[18-22, 23-45, 46+, 18-22, 23-45, 46+]</td>\n",
       "      <td>[NO, NO, NO, NO, NO, NO]</td>\n",
       "      <td>[-, -, -, -, -, -]</td>\n",
       "      <td>[[-], [-], [-], [-], [-], [-]]</td>\n",
       "      <td>DEV_ES</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>487</th>\n",
       "      <td>300539</td>\n",
       "      <td>es</td>\n",
       "      <td>En el #podcast Un día de libros 67: Violencia ...</td>\n",
       "      <td>6</td>\n",
       "      <td>[Annotator_754, Annotator_320, Annotator_123, ...</td>\n",
       "      <td>[F, F, F, M, M, M]</td>\n",
       "      <td>[18-22, 23-45, 46+, 18-22, 23-45, 46+]</td>\n",
       "      <td>[NO, YES, NO, NO, NO, NO]</td>\n",
       "      <td>[-, REPORTED, -, -, -, -]</td>\n",
       "      <td>[[-], [MISOGYNY-NON-SEXUAL-VIOLENCE], [-], [-]...</td>\n",
       "      <td>DEV_ES</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>488</th>\n",
       "      <td>300540</td>\n",
       "      <td>es</td>\n",
       "      <td>Top story: NBA tiene menos actos de violencia ...</td>\n",
       "      <td>6</td>\n",
       "      <td>[Annotator_259, Annotator_739, Annotator_291, ...</td>\n",
       "      <td>[F, F, F, M, M, M]</td>\n",
       "      <td>[18-22, 23-45, 46+, 18-22, 23-45, 46+]</td>\n",
       "      <td>[NO, NO, NO, NO, YES, NO]</td>\n",
       "      <td>[-, -, -, -, JUDGEMENTAL, -]</td>\n",
       "      <td>[[-], [-], [-], [-], [MISOGYNY-NON-SEXUAL-VIOL...</td>\n",
       "      <td>DEV_ES</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489</th>\n",
       "      <td>300542</td>\n",
       "      <td>es</td>\n",
       "      <td>@ulisesjaitt Si violó? Estuvo preso, poco tiem...</td>\n",
       "      <td>6</td>\n",
       "      <td>[Annotator_754, Annotator_320, Annotator_123, ...</td>\n",
       "      <td>[F, F, F, M, M, M]</td>\n",
       "      <td>[18-22, 23-45, 46+, 18-22, 23-45, 46+]</td>\n",
       "      <td>[NO, YES, NO, NO, NO, NO]</td>\n",
       "      <td>[-, JUDGEMENTAL, -, -, -, -]</td>\n",
       "      <td>[[-], [SEXUAL-VIOLENCE], [-], [-], [-], [-]]</td>\n",
       "      <td>DEV_ES</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>490 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    id_EXIST lang                                              tweet  \\\n",
       "0     300002   es  @anacaotica88 @MordorLivin No me acuerdo de lo...   \n",
       "1     300004   es  Also mientras les decia eso la señalaba y deci...   \n",
       "2     300007   es  @DavidGR18 @pppbernat @abc_es @agarzon @IreneM...   \n",
       "3     300008   es  @DavidArranzVox @AnabelAlonso_of Uyyy a q huel...   \n",
       "4     300010   es  @kokreto84 @Play87834898 @venusoncrack Me gust...   \n",
       "..       ...  ...                                                ...   \n",
       "485   300534   es  El Gobernador @samuel_garcias  se da el lujo d...   \n",
       "486   300538   es  Violencia simbólica también es un Presidente q...   \n",
       "487   300539   es  En el #podcast Un día de libros 67: Violencia ...   \n",
       "488   300540   es  Top story: NBA tiene menos actos de violencia ...   \n",
       "489   300542   es  @ulisesjaitt Si violó? Estuvo preso, poco tiem...   \n",
       "\n",
       "    number_annotators                                         annotators  \\\n",
       "0                   6  [Annotator_731, Annotator_732, Annotator_315, ...   \n",
       "1                   6  [Annotator_259, Annotator_739, Annotator_291, ...   \n",
       "2                   6  [Annotator_731, Annotator_732, Annotator_315, ...   \n",
       "3                   6  [Annotator_742, Annotator_743, Annotator_195, ...   \n",
       "4                   6  [Annotator_744, Annotator_745, Annotator_746, ...   \n",
       "..                ...                                                ...   \n",
       "485                 6  [Annotator_726, Annotator_727, Annotator_357, ...   \n",
       "486                 6  [Annotator_750, Annotator_751, Annotator_189, ...   \n",
       "487                 6  [Annotator_754, Annotator_320, Annotator_123, ...   \n",
       "488                 6  [Annotator_259, Annotator_739, Annotator_291, ...   \n",
       "489                 6  [Annotator_754, Annotator_320, Annotator_123, ...   \n",
       "\n",
       "      gender_annotators                          age_annotators  \\\n",
       "0    [F, F, F, M, M, M]  [18-22, 23-45, 46+, 18-22, 23-45, 46+]   \n",
       "1    [F, F, F, M, M, M]  [18-22, 23-45, 46+, 18-22, 23-45, 46+]   \n",
       "2    [F, F, F, M, M, M]  [18-22, 23-45, 46+, 18-22, 23-45, 46+]   \n",
       "3    [F, F, F, M, M, M]  [18-22, 23-45, 46+, 18-22, 23-45, 46+]   \n",
       "4    [F, F, F, M, M, M]  [18-22, 23-45, 46+, 18-22, 23-45, 46+]   \n",
       "..                  ...                                     ...   \n",
       "485  [F, F, F, M, M, M]  [18-22, 23-45, 46+, 18-22, 23-45, 46+]   \n",
       "486  [F, F, F, M, M, M]  [18-22, 23-45, 46+, 18-22, 23-45, 46+]   \n",
       "487  [F, F, F, M, M, M]  [18-22, 23-45, 46+, 18-22, 23-45, 46+]   \n",
       "488  [F, F, F, M, M, M]  [18-22, 23-45, 46+, 18-22, 23-45, 46+]   \n",
       "489  [F, F, F, M, M, M]  [18-22, 23-45, 46+, 18-22, 23-45, 46+]   \n",
       "\n",
       "                       labels_task1  \\\n",
       "0     [YES, YES, NO, YES, YES, YES]   \n",
       "1     [NO, YES, YES, YES, YES, YES]   \n",
       "2     [YES, YES, NO, YES, YES, YES]   \n",
       "3     [YES, YES, NO, YES, YES, YES]   \n",
       "4    [YES, YES, YES, YES, YES, YES]   \n",
       "..                              ...   \n",
       "485      [NO, YES, YES, NO, NO, NO]   \n",
       "486        [NO, NO, NO, NO, NO, NO]   \n",
       "487       [NO, YES, NO, NO, NO, NO]   \n",
       "488       [NO, NO, NO, NO, YES, NO]   \n",
       "489       [NO, YES, NO, NO, NO, NO]   \n",
       "\n",
       "                                          labels_task2  \\\n",
       "0    [JUDGEMENTAL, REPORTED, -, JUDGEMENTAL, JUDGEM...   \n",
       "1    [-, REPORTED, REPORTED, REPORTED, JUDGEMENTAL,...   \n",
       "2    [DIRECT, DIRECT, -, DIRECT, JUDGEMENTAL, REPOR...   \n",
       "3    [JUDGEMENTAL, JUDGEMENTAL, -, REPORTED, JUDGEM...   \n",
       "4    [DIRECT, REPORTED, DIRECT, JUDGEMENTAL, DIRECT...   \n",
       "..                                                 ...   \n",
       "485                [-, REPORTED, JUDGEMENTAL, -, -, -]   \n",
       "486                                 [-, -, -, -, -, -]   \n",
       "487                          [-, REPORTED, -, -, -, -]   \n",
       "488                       [-, -, -, -, JUDGEMENTAL, -]   \n",
       "489                       [-, JUDGEMENTAL, -, -, -, -]   \n",
       "\n",
       "                                          labels_task3   split label1  \n",
       "0    [[IDEOLOGICAL-INEQUALITY, STEREOTYPING-DOMINAN...  DEV_ES    YES  \n",
       "1    [[-], [SEXUAL-VIOLENCE], [SEXUAL-VIOLENCE], [S...  DEV_ES    YES  \n",
       "2    [[IDEOLOGICAL-INEQUALITY, SEXUAL-VIOLENCE, MIS...  DEV_ES    YES  \n",
       "3    [[IDEOLOGICAL-INEQUALITY, STEREOTYPING-DOMINAN...  DEV_ES    YES  \n",
       "4    [[MISOGYNY-NON-SEXUAL-VIOLENCE], [STEREOTYPING...  DEV_ES    YES  \n",
       "..                                                 ...     ...    ...  \n",
       "485  [[-], [IDEOLOGICAL-INEQUALITY], [STEREOTYPING-...  DEV_ES     NO  \n",
       "486                     [[-], [-], [-], [-], [-], [-]]  DEV_ES     NO  \n",
       "487  [[-], [MISOGYNY-NON-SEXUAL-VIOLENCE], [-], [-]...  DEV_ES     NO  \n",
       "488  [[-], [-], [-], [-], [MISOGYNY-NON-SEXUAL-VIOL...  DEV_ES     NO  \n",
       "489       [[-], [SEXUAL-VIOLENCE], [-], [-], [-], [-]]  DEV_ES     NO  \n",
       "\n",
       "[490 rows x 12 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frames = [validation_yes_es, validation_no_es]\n",
    "validation = pd.concat(frames)\n",
    "validation.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b4c96838-715e-4830-a73a-fd39d30354a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_EXIST</th>\n",
       "      <th>lang</th>\n",
       "      <th>tweet</th>\n",
       "      <th>number_annotators</th>\n",
       "      <th>annotators</th>\n",
       "      <th>gender_annotators</th>\n",
       "      <th>age_annotators</th>\n",
       "      <th>labels_task1</th>\n",
       "      <th>labels_task2</th>\n",
       "      <th>labels_task3</th>\n",
       "      <th>split</th>\n",
       "      <th>label1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100001</td>\n",
       "      <td>es</td>\n",
       "      <td>@TheChiflis Ignora al otro, es un capullo.El p...</td>\n",
       "      <td>6</td>\n",
       "      <td>[Annotator_1, Annotator_2, Annotator_3, Annota...</td>\n",
       "      <td>[F, F, F, M, M, M]</td>\n",
       "      <td>[18-22, 23-45, 46+, 46+, 23-45, 18-22]</td>\n",
       "      <td>[YES, YES, NO, YES, YES, YES]</td>\n",
       "      <td>[REPORTED, JUDGEMENTAL, -, REPORTED, JUDGEMENT...</td>\n",
       "      <td>[[OBJECTIFICATION], [OBJECTIFICATION, SEXUAL-V...</td>\n",
       "      <td>TRAIN_ES</td>\n",
       "      <td>YES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100005</td>\n",
       "      <td>es</td>\n",
       "      <td>@novadragon21 @icep4ck @TvDannyZ Entonces como...</td>\n",
       "      <td>6</td>\n",
       "      <td>[Annotator_19, Annotator_20, Annotator_21, Ann...</td>\n",
       "      <td>[F, F, F, M, M, M]</td>\n",
       "      <td>[18-22, 23-45, 46+, 46+, 23-45, 18-22]</td>\n",
       "      <td>[YES, NO, YES, NO, YES, YES]</td>\n",
       "      <td>[REPORTED, -, JUDGEMENTAL, -, JUDGEMENTAL, DIR...</td>\n",
       "      <td>[[STEREOTYPING-DOMINANCE, OBJECTIFICATION], [-...</td>\n",
       "      <td>TRAIN_ES</td>\n",
       "      <td>YES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100008</td>\n",
       "      <td>es</td>\n",
       "      <td>@BestKabest Esta gringa sigue llorando por el ...</td>\n",
       "      <td>6</td>\n",
       "      <td>[Annotator_25, Annotator_26, Annotator_27, Ann...</td>\n",
       "      <td>[F, F, F, M, M, M]</td>\n",
       "      <td>[18-22, 23-45, 46+, 46+, 23-45, 18-22]</td>\n",
       "      <td>[YES, YES, YES, YES, YES, YES]</td>\n",
       "      <td>[DIRECT, DIRECT, DIRECT, JUDGEMENTAL, DIRECT, ...</td>\n",
       "      <td>[[IDEOLOGICAL-INEQUALITY], [STEREOTYPING-DOMIN...</td>\n",
       "      <td>TRAIN_ES</td>\n",
       "      <td>YES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100028</td>\n",
       "      <td>es</td>\n",
       "      <td>@ShahidForChange @TeamPelosi Quiet, sexist ^%$...</td>\n",
       "      <td>6</td>\n",
       "      <td>[Annotator_109, Annotator_110, Annotator_111, ...</td>\n",
       "      <td>[F, F, F, M, M, M]</td>\n",
       "      <td>[18-22, 23-45, 46+, 46+, 23-45, 18-22]</td>\n",
       "      <td>[YES, NO, NO, YES, YES, YES]</td>\n",
       "      <td>[JUDGEMENTAL, -, -, DIRECT, DIRECT, DIRECT]</td>\n",
       "      <td>[[STEREOTYPING-DOMINANCE], [-], [-], [IDEOLOGI...</td>\n",
       "      <td>TRAIN_ES</td>\n",
       "      <td>YES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100036</td>\n",
       "      <td>es</td>\n",
       "      <td>@Harassed_girl loca d mierda en k momento</td>\n",
       "      <td>6</td>\n",
       "      <td>[Annotator_115, Annotator_116, Annotator_117, ...</td>\n",
       "      <td>[F, F, F, M, M, M]</td>\n",
       "      <td>[18-22, 23-45, 46+, 46+, 23-45, 18-22]</td>\n",
       "      <td>[YES, YES, YES, YES, YES, NO]</td>\n",
       "      <td>[DIRECT, DIRECT, DIRECT, DIRECT, DIRECT, -]</td>\n",
       "      <td>[[STEREOTYPING-DOMINANCE, SEXUAL-VIOLENCE], [M...</td>\n",
       "      <td>TRAIN_ES</td>\n",
       "      <td>YES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3655</th>\n",
       "      <td>103618</td>\n",
       "      <td>es</td>\n",
       "      <td>SUJETO QUE VIOLÓ A UNA MENOR ES SENTENCIADO A ...</td>\n",
       "      <td>6</td>\n",
       "      <td>[Annotator_205, Annotator_206, Annotator_207, ...</td>\n",
       "      <td>[F, F, F, M, M, M]</td>\n",
       "      <td>[18-22, 23-45, 46+, 46+, 23-45, 18-22]</td>\n",
       "      <td>[YES, NO, NO, NO, YES, YES]</td>\n",
       "      <td>[REPORTED, -, -, -, REPORTED, REPORTED]</td>\n",
       "      <td>[[SEXUAL-VIOLENCE], [-], [-], [-], [SEXUAL-VIO...</td>\n",
       "      <td>TRAIN_ES</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3656</th>\n",
       "      <td>103625</td>\n",
       "      <td>es</td>\n",
       "      <td>- Aaaaaaaa• La Zorra abria la boca y se estamp...</td>\n",
       "      <td>6</td>\n",
       "      <td>[Annotator_175, Annotator_176, Annotator_177, ...</td>\n",
       "      <td>[F, F, F, M, M, M]</td>\n",
       "      <td>[18-22, 23-45, 46+, 46+, 23-45, 18-22]</td>\n",
       "      <td>[YES, NO, NO, YES, YES, NO]</td>\n",
       "      <td>[DIRECT, -, -, DIRECT, REPORTED, -]</td>\n",
       "      <td>[[OBJECTIFICATION], [-], [-], [OBJECTIFICATION...</td>\n",
       "      <td>TRAIN_ES</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3657</th>\n",
       "      <td>103629</td>\n",
       "      <td>es</td>\n",
       "      <td>@strangrbrina ahí te contesto PERO YOCSOY TU A...</td>\n",
       "      <td>6</td>\n",
       "      <td>[Annotator_343, Annotator_344, Annotator_345, ...</td>\n",
       "      <td>[F, F, F, M, M, M]</td>\n",
       "      <td>[18-22, 23-45, 46+, 46+, 23-45, 18-22]</td>\n",
       "      <td>[YES, YES, NO, NO, YES, NO]</td>\n",
       "      <td>[DIRECT, DIRECT, -, -, DIRECT, -]</td>\n",
       "      <td>[[MISOGYNY-NON-SEXUAL-VIOLENCE], [OBJECTIFICAT...</td>\n",
       "      <td>TRAIN_ES</td>\n",
       "      <td>YES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3658</th>\n",
       "      <td>103634</td>\n",
       "      <td>es</td>\n",
       "      <td>@javierLander23 Que vai a ser vieja eskuela vo...</td>\n",
       "      <td>6</td>\n",
       "      <td>[Annotator_199, Annotator_200, Annotator_201, ...</td>\n",
       "      <td>[F, F, F, M, M, M]</td>\n",
       "      <td>[18-22, 23-45, 46+, 46+, 23-45, 18-22]</td>\n",
       "      <td>[YES, YES, YES, NO, NO, NO]</td>\n",
       "      <td>[JUDGEMENTAL, REPORTED, DIRECT, -, -, -]</td>\n",
       "      <td>[[IDEOLOGICAL-INEQUALITY], [STEREOTYPING-DOMIN...</td>\n",
       "      <td>TRAIN_ES</td>\n",
       "      <td>YES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3659</th>\n",
       "      <td>103659</td>\n",
       "      <td>es</td>\n",
       "      <td>@patofigueroam Las 2 estrellas que se bordó SW...</td>\n",
       "      <td>6</td>\n",
       "      <td>[Annotator_139, Annotator_140, Annotator_141, ...</td>\n",
       "      <td>[F, F, F, M, M, M]</td>\n",
       "      <td>[18-22, 23-45, 46+, 46+, 23-45, 18-22]</td>\n",
       "      <td>[NO, NO, YES, NO, YES, YES]</td>\n",
       "      <td>[-, -, DIRECT, -, JUDGEMENTAL, DIRECT]</td>\n",
       "      <td>[[-], [-], [OBJECTIFICATION, SEXUAL-VIOLENCE, ...</td>\n",
       "      <td>TRAIN_ES</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3660 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     id_EXIST lang                                              tweet  \\\n",
       "0      100001   es  @TheChiflis Ignora al otro, es un capullo.El p...   \n",
       "1      100005   es  @novadragon21 @icep4ck @TvDannyZ Entonces como...   \n",
       "2      100008   es  @BestKabest Esta gringa sigue llorando por el ...   \n",
       "3      100028   es  @ShahidForChange @TeamPelosi Quiet, sexist ^%$...   \n",
       "4      100036   es          @Harassed_girl loca d mierda en k momento   \n",
       "...       ...  ...                                                ...   \n",
       "3655   103618   es  SUJETO QUE VIOLÓ A UNA MENOR ES SENTENCIADO A ...   \n",
       "3656   103625   es  - Aaaaaaaa• La Zorra abria la boca y se estamp...   \n",
       "3657   103629   es  @strangrbrina ahí te contesto PERO YOCSOY TU A...   \n",
       "3658   103634   es  @javierLander23 Que vai a ser vieja eskuela vo...   \n",
       "3659   103659   es  @patofigueroam Las 2 estrellas que se bordó SW...   \n",
       "\n",
       "     number_annotators                                         annotators  \\\n",
       "0                    6  [Annotator_1, Annotator_2, Annotator_3, Annota...   \n",
       "1                    6  [Annotator_19, Annotator_20, Annotator_21, Ann...   \n",
       "2                    6  [Annotator_25, Annotator_26, Annotator_27, Ann...   \n",
       "3                    6  [Annotator_109, Annotator_110, Annotator_111, ...   \n",
       "4                    6  [Annotator_115, Annotator_116, Annotator_117, ...   \n",
       "...                ...                                                ...   \n",
       "3655                 6  [Annotator_205, Annotator_206, Annotator_207, ...   \n",
       "3656                 6  [Annotator_175, Annotator_176, Annotator_177, ...   \n",
       "3657                 6  [Annotator_343, Annotator_344, Annotator_345, ...   \n",
       "3658                 6  [Annotator_199, Annotator_200, Annotator_201, ...   \n",
       "3659                 6  [Annotator_139, Annotator_140, Annotator_141, ...   \n",
       "\n",
       "       gender_annotators                          age_annotators  \\\n",
       "0     [F, F, F, M, M, M]  [18-22, 23-45, 46+, 46+, 23-45, 18-22]   \n",
       "1     [F, F, F, M, M, M]  [18-22, 23-45, 46+, 46+, 23-45, 18-22]   \n",
       "2     [F, F, F, M, M, M]  [18-22, 23-45, 46+, 46+, 23-45, 18-22]   \n",
       "3     [F, F, F, M, M, M]  [18-22, 23-45, 46+, 46+, 23-45, 18-22]   \n",
       "4     [F, F, F, M, M, M]  [18-22, 23-45, 46+, 46+, 23-45, 18-22]   \n",
       "...                  ...                                     ...   \n",
       "3655  [F, F, F, M, M, M]  [18-22, 23-45, 46+, 46+, 23-45, 18-22]   \n",
       "3656  [F, F, F, M, M, M]  [18-22, 23-45, 46+, 46+, 23-45, 18-22]   \n",
       "3657  [F, F, F, M, M, M]  [18-22, 23-45, 46+, 46+, 23-45, 18-22]   \n",
       "3658  [F, F, F, M, M, M]  [18-22, 23-45, 46+, 46+, 23-45, 18-22]   \n",
       "3659  [F, F, F, M, M, M]  [18-22, 23-45, 46+, 46+, 23-45, 18-22]   \n",
       "\n",
       "                        labels_task1  \\\n",
       "0      [YES, YES, NO, YES, YES, YES]   \n",
       "1       [YES, NO, YES, NO, YES, YES]   \n",
       "2     [YES, YES, YES, YES, YES, YES]   \n",
       "3       [YES, NO, NO, YES, YES, YES]   \n",
       "4      [YES, YES, YES, YES, YES, NO]   \n",
       "...                              ...   \n",
       "3655     [YES, NO, NO, NO, YES, YES]   \n",
       "3656     [YES, NO, NO, YES, YES, NO]   \n",
       "3657     [YES, YES, NO, NO, YES, NO]   \n",
       "3658     [YES, YES, YES, NO, NO, NO]   \n",
       "3659     [NO, NO, YES, NO, YES, YES]   \n",
       "\n",
       "                                           labels_task2  \\\n",
       "0     [REPORTED, JUDGEMENTAL, -, REPORTED, JUDGEMENT...   \n",
       "1     [REPORTED, -, JUDGEMENTAL, -, JUDGEMENTAL, DIR...   \n",
       "2     [DIRECT, DIRECT, DIRECT, JUDGEMENTAL, DIRECT, ...   \n",
       "3           [JUDGEMENTAL, -, -, DIRECT, DIRECT, DIRECT]   \n",
       "4           [DIRECT, DIRECT, DIRECT, DIRECT, DIRECT, -]   \n",
       "...                                                 ...   \n",
       "3655            [REPORTED, -, -, -, REPORTED, REPORTED]   \n",
       "3656                [DIRECT, -, -, DIRECT, REPORTED, -]   \n",
       "3657                  [DIRECT, DIRECT, -, -, DIRECT, -]   \n",
       "3658           [JUDGEMENTAL, REPORTED, DIRECT, -, -, -]   \n",
       "3659             [-, -, DIRECT, -, JUDGEMENTAL, DIRECT]   \n",
       "\n",
       "                                           labels_task3     split label1  \n",
       "0     [[OBJECTIFICATION], [OBJECTIFICATION, SEXUAL-V...  TRAIN_ES    YES  \n",
       "1     [[STEREOTYPING-DOMINANCE, OBJECTIFICATION], [-...  TRAIN_ES    YES  \n",
       "2     [[IDEOLOGICAL-INEQUALITY], [STEREOTYPING-DOMIN...  TRAIN_ES    YES  \n",
       "3     [[STEREOTYPING-DOMINANCE], [-], [-], [IDEOLOGI...  TRAIN_ES    YES  \n",
       "4     [[STEREOTYPING-DOMINANCE, SEXUAL-VIOLENCE], [M...  TRAIN_ES    YES  \n",
       "...                                                 ...       ...    ...  \n",
       "3655  [[SEXUAL-VIOLENCE], [-], [-], [-], [SEXUAL-VIO...  TRAIN_ES     NO  \n",
       "3656  [[OBJECTIFICATION], [-], [-], [OBJECTIFICATION...  TRAIN_ES     NO  \n",
       "3657  [[MISOGYNY-NON-SEXUAL-VIOLENCE], [OBJECTIFICAT...  TRAIN_ES    YES  \n",
       "3658  [[IDEOLOGICAL-INEQUALITY], [STEREOTYPING-DOMIN...  TRAIN_ES    YES  \n",
       "3659  [[-], [-], [OBJECTIFICATION, SEXUAL-VIOLENCE, ...  TRAIN_ES     NO  \n",
       "\n",
       "[3660 rows x 12 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frames = [train_yes_es, train_no_es,train_amb]\n",
    "train = pd.concat(frames)\n",
    "train.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b2ff70b8-1bde-4164-97da-2bac98580b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "hash_regex = re.compile(r\"#(\\w+)\")\n",
    "hstgs = [] # To store the hashtags so we can exclude them from some parts of the analysis\n",
    "def hash_repl(match):\n",
    "    _ = '__HASH_'+match.group(1).upper()\n",
    "    hstgs.append(_)\n",
    "    return _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "00f83f9f-5ab5-4ffa-a84b-98a5f46d5172",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_regex = re.compile(r\"@(\\w+)\")\n",
    "usr_names = [] # To store the user names so we can exclude them from some parts of the analysis\n",
    "def user_repl(match):\n",
    "    _ = '__user_'+match.group(1).upper()\n",
    "    usr_names.append(_)\n",
    "    return _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "54cc094b-1626-4cf7-8822-64287e8029cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_regex = re.compile(r\"(http|https|ftp)://[a-zA-Z0-9\\./]+\")\n",
    "def url_repl(match):\n",
    "    return '__URL_'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a9d0a701-91df-4920-9dde-c0eb5d665359",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeating words like hurrrryyyyyy\n",
    "rpt_regex = re.compile(r\"(.)\\1{1,}\", re.IGNORECASE);\n",
    "def rpt_repl(match):\n",
    "    return match.group(1)+match.group(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2ab24dbc-451e-40ee-97d1-769b019aa2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spliting by word boundaries\n",
    "word_bound_regex = re.compile(r\"\\W+\")\n",
    "\n",
    "# Punctuations\n",
    "punctuations = \\\n",
    "\t[\t#('',\t\t['.', ] )\t,\\\n",
    "\t\t#('',\t\t[',', ] )\t,\\\n",
    "\t\t#('',\t\t['\\'', '\\\"', ] )\t,\\\n",
    "\t\t('__PUNC_EXCL',\t\t['!', '¡', ] )\t,\\\n",
    "\t\t('__PUNC_QUES',\t\t['?', '¿', ] )\t,\\\n",
    "\t\t('__PUNC_ELLP',\t\t['...', '…', ] )\t,\\\n",
    "\t]\n",
    "\n",
    "#For punctuation replacement\n",
    "def punctuations_repl(match):\n",
    "\ttext = match.group(0)\n",
    "\trepl = []\n",
    "\tfor (key, parr) in punctuations :\n",
    "\t\tfor punc in parr :\n",
    "\t\t\tif punc in text:\n",
    "\t\t\t\trepl.append(key)\n",
    "\tif( len(repl)>0 ) :\n",
    "\t\treturn ' '+' '.join(repl)+' '\n",
    "\telse :\n",
    "\t\treturn ' '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c3c1f1e4-ea63-4a41-99d2-dfe4b6ee86ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk.stem import SnowballStemmer\n",
    "stemmer = SnowballStemmer('spanish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f0e28fcc-769b-41de-a749-41c1de30e6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sb_stem(text, only_first=0):\n",
    "    text = [word if(word[0:2]=='__') else word.lower() for word in text.split() if ((len(word) >= 3) or (word in ['no','si', 'sí', 'ni']))] #   If we are doing negation analysis, maybe is a better idea to keep the small words (like 'no')\n",
    "    text = [stemmer.stem(w) if w[0:2]!='__' else w for w in text ]\n",
    "    \n",
    "    return(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bdba3594-c4f4-4a7c-810f-6d261466113f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def processAll(text):\n",
    "    text = re.sub( hash_regex, hash_repl, text )\n",
    "    #text = re.sub( user_regex, user_repl, text)\n",
    "    text = re.sub( url_regex, url_repl, text )\n",
    "    \n",
    "    text = text.replace('\\'','')\n",
    "    \n",
    "    text = re.sub( word_bound_regex , punctuations_repl, text )\n",
    "    text = re.sub( rpt_regex, rpt_repl, text )\n",
    "    \n",
    "    text = sb_stem(text)    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c828b58d-402a-47a4-bab8-bbb46786110d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['processed_tweet'] = train.tweet.apply(processAll)\n",
    "validation['processed_tweet'] = validation.tweet.apply(processAll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd76f8df-6cfd-45cc-8309-a48b32120f0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8b477e96-3382-4cda-ae81-87e680ce0445",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_EXIST</th>\n",
       "      <th>lang</th>\n",
       "      <th>tweet</th>\n",
       "      <th>number_annotators</th>\n",
       "      <th>annotators</th>\n",
       "      <th>gender_annotators</th>\n",
       "      <th>age_annotators</th>\n",
       "      <th>labels_task1</th>\n",
       "      <th>labels_task2</th>\n",
       "      <th>labels_task3</th>\n",
       "      <th>split</th>\n",
       "      <th>label1</th>\n",
       "      <th>processed_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>100001</th>\n",
       "      <td>100001</td>\n",
       "      <td>es</td>\n",
       "      <td>@TheChiflis Ignora al otro, es un capullo.El p...</td>\n",
       "      <td>6</td>\n",
       "      <td>[Annotator_1, Annotator_2, Annotator_3, Annota...</td>\n",
       "      <td>[F, F, F, M, M, M]</td>\n",
       "      <td>[18-22, 23-45, 46+, 46+, 23-45, 18-22]</td>\n",
       "      <td>[YES, YES, NO, YES, YES, YES]</td>\n",
       "      <td>[REPORTED, JUDGEMENTAL, -, REPORTED, JUDGEMENT...</td>\n",
       "      <td>[[OBJECTIFICATION], [OBJECTIFICATION, SEXUAL-V...</td>\n",
       "      <td>TRAIN_ES</td>\n",
       "      <td>YES</td>\n",
       "      <td>[thechiflis, ignor, otro, capull, problem, con...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100005</th>\n",
       "      <td>100005</td>\n",
       "      <td>es</td>\n",
       "      <td>@novadragon21 @icep4ck @TvDannyZ Entonces como...</td>\n",
       "      <td>6</td>\n",
       "      <td>[Annotator_19, Annotator_20, Annotator_21, Ann...</td>\n",
       "      <td>[F, F, F, M, M, M]</td>\n",
       "      <td>[18-22, 23-45, 46+, 46+, 23-45, 18-22]</td>\n",
       "      <td>[YES, NO, YES, NO, YES, YES]</td>\n",
       "      <td>[REPORTED, -, JUDGEMENTAL, -, JUDGEMENTAL, DIR...</td>\n",
       "      <td>[[STEREOTYPING-DOMINANCE, OBJECTIFICATION], [-...</td>\n",
       "      <td>TRAIN_ES</td>\n",
       "      <td>YES</td>\n",
       "      <td>[novadragon21, icep4ck, tvdannyz, entonc, com,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100008</th>\n",
       "      <td>100008</td>\n",
       "      <td>es</td>\n",
       "      <td>@BestKabest Esta gringa sigue llorando por el ...</td>\n",
       "      <td>6</td>\n",
       "      <td>[Annotator_25, Annotator_26, Annotator_27, Ann...</td>\n",
       "      <td>[F, F, F, M, M, M]</td>\n",
       "      <td>[18-22, 23-45, 46+, 46+, 23-45, 18-22]</td>\n",
       "      <td>[YES, YES, YES, YES, YES, YES]</td>\n",
       "      <td>[DIRECT, DIRECT, DIRECT, JUDGEMENTAL, DIRECT, ...</td>\n",
       "      <td>[[IDEOLOGICAL-INEQUALITY], [STEREOTYPING-DOMIN...</td>\n",
       "      <td>TRAIN_ES</td>\n",
       "      <td>YES</td>\n",
       "      <td>[bestkabest, esta, gring, sig, llor, por, game...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100028</th>\n",
       "      <td>100028</td>\n",
       "      <td>es</td>\n",
       "      <td>@ShahidForChange @TeamPelosi Quiet, sexist ^%$...</td>\n",
       "      <td>6</td>\n",
       "      <td>[Annotator_109, Annotator_110, Annotator_111, ...</td>\n",
       "      <td>[F, F, F, M, M, M]</td>\n",
       "      <td>[18-22, 23-45, 46+, 46+, 23-45, 18-22]</td>\n",
       "      <td>[YES, NO, NO, YES, YES, YES]</td>\n",
       "      <td>[JUDGEMENTAL, -, -, DIRECT, DIRECT, DIRECT]</td>\n",
       "      <td>[[STEREOTYPING-DOMINANCE], [-], [-], [IDEOLOGI...</td>\n",
       "      <td>TRAIN_ES</td>\n",
       "      <td>YES</td>\n",
       "      <td>[shahidforchang, teampelosi, quiet, sexist, __...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100036</th>\n",
       "      <td>100036</td>\n",
       "      <td>es</td>\n",
       "      <td>@Harassed_girl loca d mierda en k momento</td>\n",
       "      <td>6</td>\n",
       "      <td>[Annotator_115, Annotator_116, Annotator_117, ...</td>\n",
       "      <td>[F, F, F, M, M, M]</td>\n",
       "      <td>[18-22, 23-45, 46+, 46+, 23-45, 18-22]</td>\n",
       "      <td>[YES, YES, YES, YES, YES, NO]</td>\n",
       "      <td>[DIRECT, DIRECT, DIRECT, DIRECT, DIRECT, -]</td>\n",
       "      <td>[[STEREOTYPING-DOMINANCE, SEXUAL-VIOLENCE], [M...</td>\n",
       "      <td>TRAIN_ES</td>\n",
       "      <td>YES</td>\n",
       "      <td>[harassed_girl, loc, mierd, moment]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id_EXIST lang                                              tweet  \\\n",
       "100001   100001   es  @TheChiflis Ignora al otro, es un capullo.El p...   \n",
       "100005   100005   es  @novadragon21 @icep4ck @TvDannyZ Entonces como...   \n",
       "100008   100008   es  @BestKabest Esta gringa sigue llorando por el ...   \n",
       "100028   100028   es  @ShahidForChange @TeamPelosi Quiet, sexist ^%$...   \n",
       "100036   100036   es          @Harassed_girl loca d mierda en k momento   \n",
       "\n",
       "       number_annotators                                         annotators  \\\n",
       "100001                 6  [Annotator_1, Annotator_2, Annotator_3, Annota...   \n",
       "100005                 6  [Annotator_19, Annotator_20, Annotator_21, Ann...   \n",
       "100008                 6  [Annotator_25, Annotator_26, Annotator_27, Ann...   \n",
       "100028                 6  [Annotator_109, Annotator_110, Annotator_111, ...   \n",
       "100036                 6  [Annotator_115, Annotator_116, Annotator_117, ...   \n",
       "\n",
       "         gender_annotators                          age_annotators  \\\n",
       "100001  [F, F, F, M, M, M]  [18-22, 23-45, 46+, 46+, 23-45, 18-22]   \n",
       "100005  [F, F, F, M, M, M]  [18-22, 23-45, 46+, 46+, 23-45, 18-22]   \n",
       "100008  [F, F, F, M, M, M]  [18-22, 23-45, 46+, 46+, 23-45, 18-22]   \n",
       "100028  [F, F, F, M, M, M]  [18-22, 23-45, 46+, 46+, 23-45, 18-22]   \n",
       "100036  [F, F, F, M, M, M]  [18-22, 23-45, 46+, 46+, 23-45, 18-22]   \n",
       "\n",
       "                          labels_task1  \\\n",
       "100001   [YES, YES, NO, YES, YES, YES]   \n",
       "100005    [YES, NO, YES, NO, YES, YES]   \n",
       "100008  [YES, YES, YES, YES, YES, YES]   \n",
       "100028    [YES, NO, NO, YES, YES, YES]   \n",
       "100036   [YES, YES, YES, YES, YES, NO]   \n",
       "\n",
       "                                             labels_task2  \\\n",
       "100001  [REPORTED, JUDGEMENTAL, -, REPORTED, JUDGEMENT...   \n",
       "100005  [REPORTED, -, JUDGEMENTAL, -, JUDGEMENTAL, DIR...   \n",
       "100008  [DIRECT, DIRECT, DIRECT, JUDGEMENTAL, DIRECT, ...   \n",
       "100028        [JUDGEMENTAL, -, -, DIRECT, DIRECT, DIRECT]   \n",
       "100036        [DIRECT, DIRECT, DIRECT, DIRECT, DIRECT, -]   \n",
       "\n",
       "                                             labels_task3     split label1  \\\n",
       "100001  [[OBJECTIFICATION], [OBJECTIFICATION, SEXUAL-V...  TRAIN_ES    YES   \n",
       "100005  [[STEREOTYPING-DOMINANCE, OBJECTIFICATION], [-...  TRAIN_ES    YES   \n",
       "100008  [[IDEOLOGICAL-INEQUALITY], [STEREOTYPING-DOMIN...  TRAIN_ES    YES   \n",
       "100028  [[STEREOTYPING-DOMINANCE], [-], [-], [IDEOLOGI...  TRAIN_ES    YES   \n",
       "100036  [[STEREOTYPING-DOMINANCE, SEXUAL-VIOLENCE], [M...  TRAIN_ES    YES   \n",
       "\n",
       "                                          processed_tweet  \n",
       "100001  [thechiflis, ignor, otro, capull, problem, con...  \n",
       "100005  [novadragon21, icep4ck, tvdannyz, entonc, com,...  \n",
       "100008  [bestkabest, esta, gring, sig, llor, por, game...  \n",
       "100028  [shahidforchang, teampelosi, quiet, sexist, __...  \n",
       "100036                [harassed_girl, loc, mierd, moment]  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3b8229af-5964-4870-a770-e39561aa67a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[thechiflis, ignor, otro, capull, problem, con...</td>\n",
       "      <td>YES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[novadragon21, icep4ck, tvdannyz, entonc, com,...</td>\n",
       "      <td>YES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[bestkabest, esta, gring, sig, llor, por, game...</td>\n",
       "      <td>YES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[shahidforchang, teampelosi, quiet, sexist, __...</td>\n",
       "      <td>YES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[harassed_girl, loc, mierd, moment]</td>\n",
       "      <td>YES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3655</th>\n",
       "      <td>[sujet, que, viol, una, menor, sentenci, años,...</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3656</th>\n",
       "      <td>[zorr, abri, boc, estamp, contr, intim, del, a...</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3657</th>\n",
       "      <td>[strangrbrin, ahi, contest, per, yocsoy, amig,...</td>\n",
       "      <td>YES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3658</th>\n",
       "      <td>[javierlander23, que, vai, ser, viej, eskuel, ...</td>\n",
       "      <td>YES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3659</th>\n",
       "      <td>[patofigueroam, las, estrell, que, bord, las, ...</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3660 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  tweet label\n",
       "0     [thechiflis, ignor, otro, capull, problem, con...   YES\n",
       "1     [novadragon21, icep4ck, tvdannyz, entonc, com,...   YES\n",
       "2     [bestkabest, esta, gring, sig, llor, por, game...   YES\n",
       "3     [shahidforchang, teampelosi, quiet, sexist, __...   YES\n",
       "4                   [harassed_girl, loc, mierd, moment]   YES\n",
       "...                                                 ...   ...\n",
       "3655  [sujet, que, viol, una, menor, sentenci, años,...    NO\n",
       "3656  [zorr, abri, boc, estamp, contr, intim, del, a...    NO\n",
       "3657  [strangrbrin, ahi, contest, per, yocsoy, amig,...   YES\n",
       "3658  [javierlander23, que, vai, ser, viej, eskuel, ...   YES\n",
       "3659  [patofigueroam, las, estrell, que, bord, las, ...    NO\n",
       "\n",
       "[3660 rows x 2 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.DataFrame()\n",
    "train_data['tweet'] = train['processed_tweet']\n",
    "train_data['label'] = train['label1']\n",
    "train_data.reset_index(inplace=True, drop = True)\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2e6a3358-b508-4a24-81f0-5d8a56bcf561",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[anacaotica88, mordorlivin, acuerd, los, detal...</td>\n",
       "      <td>YES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[also, mientr, les, deci, eso, señal, deci, qu...</td>\n",
       "      <td>YES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[davidgr18, ppbernat, abc_es, agarzon, irenemo...</td>\n",
       "      <td>YES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[davidarranzvox, anabelalonso_of, uyy, huel, _...</td>\n",
       "      <td>YES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[kokreto84, play87834898, venusoncrack, gust, ...</td>\n",
       "      <td>YES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>485</th>\n",
       "      <td>[gobern, samuel_garci, luj, sub, vide, complet...</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>486</th>\n",
       "      <td>[violenci, simbol, tambien, president, que, to...</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>487</th>\n",
       "      <td>[__HASH_PODCAST, dia, libr, violenci, simbol, ...</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>488</th>\n",
       "      <td>[top, story, nba, tien, men, actos, violenci, ...</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489</th>\n",
       "      <td>[ulisesjaitt, viol, __PUNC_QUES, estuv, pres, ...</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>490 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 tweet label\n",
       "0    [anacaotica88, mordorlivin, acuerd, los, detal...   YES\n",
       "1    [also, mientr, les, deci, eso, señal, deci, qu...   YES\n",
       "2    [davidgr18, ppbernat, abc_es, agarzon, irenemo...   YES\n",
       "3    [davidarranzvox, anabelalonso_of, uyy, huel, _...   YES\n",
       "4    [kokreto84, play87834898, venusoncrack, gust, ...   YES\n",
       "..                                                 ...   ...\n",
       "485  [gobern, samuel_garci, luj, sub, vide, complet...    NO\n",
       "486  [violenci, simbol, tambien, president, que, to...    NO\n",
       "487  [__HASH_PODCAST, dia, libr, violenci, simbol, ...    NO\n",
       "488  [top, story, nba, tien, men, actos, violenci, ...    NO\n",
       "489  [ulisesjaitt, viol, __PUNC_QUES, estuv, pres, ...    NO\n",
       "\n",
       "[490 rows x 2 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_data = pd.DataFrame()\n",
    "validation_data['tweet'] = validation['processed_tweet']\n",
    "validation_data['label'] = validation['label1']\n",
    "validation_data.reset_index(inplace=True, drop = True)\n",
    "validation_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "58d3ec08-fd0a-48b9-9fca-d6a20c7fa6f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "48\n"
     ]
    }
   ],
   "source": [
    "print(max(train_data['tweet'].apply(len)))\n",
    "print(max(validation_data['tweet'].apply(len)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eacb49e5-72a6-473f-8021-e82a11ea6d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0bfedd40-6b1c-4508-94c6-c4b313f56ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "vecs = KeyedVectors.load_word2vec_format('SBW-vectors-300-min5.txt', binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cedc258b-2aac-4c4f-bc59-5cfb53546fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_tweet(tweet):\n",
    "    vec = []\n",
    "    for word in tweet:\n",
    "        if word in vecs.key_to_index:\n",
    "            vec.append(vecs[word])\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "88bf3bd8-f36c-42d2-b45d-aac01251ac9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['tweet'] = train_data['tweet'].apply(vectorize_tweet)\n",
    "validation_data['tweet'] = validation_data['tweet'].apply(vectorize_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f72fdf2e-7053-4360-9d1f-0c3c6b841d18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n",
      "43\n"
     ]
    }
   ],
   "source": [
    "print(max(train_data['tweet'].apply(len)))\n",
    "print(max(validation_data['tweet'].apply(len)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b3834030-2173-4d5d-9b76-4c39b2ad0306",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "X_train = list(train_data['tweet'])\n",
    "Y_train = list(train_data['label'])\n",
    "\n",
    "X_test = list(validation_data['tweet'])\n",
    "Y_test = list(validation_data['label'])\n",
    "\n",
    "temp = list(zip(X_train, Y_train))\n",
    "random.shuffle(temp)\n",
    "X_train, Y_train = zip(*temp)\n",
    "X_train = list(X_train)\n",
    "Y_train = list(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "880a2f34-3187-4cbd-9ef9-7e516bf57797",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2a5de1f2-2d6d-486e-88a6-9d9dacf3632f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def padto43(x):\n",
    "    if len(x) < 43:\n",
    "        for i in range(43 - len(x)):\n",
    "            x.append([0]*300)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4019fce7-8682-4a07-b288-c85eff9710b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(X_train)):\n",
    "    X_train[i] = padto43(X_train[i])\n",
    "\n",
    "for i in range(len(X_test)):\n",
    "    X_test[i] = padto43(X_test[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a0f48f8b-92c3-4280-a6be-f9a1edaa39d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-04 01:47:39.421956: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-05-04 01:47:39.422528: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1956] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2023-05-04 01:47:39.423686: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 188856000 exceeds 10% of free system memory.\n"
     ]
    }
   ],
   "source": [
    "X_train_tensor = tf.convert_to_tensor(X_train)\n",
    "X_test_tensor = tf.convert_to_tensor(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a15bcf68-db87-40d4-a99c-69d582df39d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([3660   43  300], shape=(3,), dtype=int32)\n",
      "tf.Tensor([490  43 300], shape=(3,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "print(tf.shape(X_train_tensor))\n",
    "print(tf.shape(X_test_tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a0596398-8e47-4ae7-9861-8d95d6c5e1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert yes to 1 and no to 0\n",
    "for i in range(len(Y_train)):\n",
    "    if Y_train[i] == 'YES':\n",
    "        Y_train[i] = [1, 0]\n",
    "    else:\n",
    "        Y_train[i] = [0, 1]\n",
    "\n",
    "for i in range(len(Y_test)):\n",
    "    if Y_test[i] == 'YES':\n",
    "        Y_test[i] = [1, 0]\n",
    "    else:\n",
    "        Y_test[i] = [0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "107288fb-040c-4fe9-a6bc-9224e900a56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Y_train_tensor = tf.convert_to_tensor(Y_train)\n",
    "Y_test_tensor = tf.convert_to_tensor(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b5cf3ddd-cbdb-4856-ad6a-3d0ff4323a89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 150)               270600    \n",
      "                                                                 \n",
      " dense (Dense)               (None, 2)                 302       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 270,902\n",
      "Trainable params: 270,902\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-04 01:47:43.121363: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-05-04 01:47:43.122961: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-05-04 01:47:43.124213: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    }
   ],
   "source": [
    "MLmodel = tf.keras.Sequential([\n",
    "    tf.keras.layers.LSTM(150),\n",
    "    tf.keras.layers.Dense(2, activation='softmax')\n",
    "])\n",
    "MLmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "MLmodel.build(input_shape=(None, 43, 300))\n",
    "MLmodel.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "34e37d16-d005-4ee9-8ede-d301e842da8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-04 01:47:43.494651: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-05-04 01:47:43.496335: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-05-04 01:47:43.497693: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-05-04 01:47:44.158175: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-05-04 01:47:44.159834: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-05-04 01:47:44.161174: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115/115 [==============================] - ETA: 0s - loss: 0.6931 - accuracy: 0.5046"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-04 01:47:48.425856: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-05-04 01:47:48.427400: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-05-04 01:47:48.428669: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115/115 [==============================] - 6s 32ms/step - loss: 0.6931 - accuracy: 0.5046 - val_loss: 0.6933 - val_accuracy: 0.4918\n",
      "Epoch 2/50\n",
      "115/115 [==============================] - 3s 28ms/step - loss: 0.6933 - accuracy: 0.5183 - val_loss: 0.6939 - val_accuracy: 0.4776\n",
      "Epoch 3/50\n",
      "115/115 [==============================] - 3s 28ms/step - loss: 0.6876 - accuracy: 0.5552 - val_loss: 0.6655 - val_accuracy: 0.5776\n",
      "Epoch 4/50\n",
      "115/115 [==============================] - 3s 28ms/step - loss: 0.6910 - accuracy: 0.5265 - val_loss: 0.6929 - val_accuracy: 0.5000\n",
      "Epoch 5/50\n",
      "115/115 [==============================] - 3s 28ms/step - loss: 0.6805 - accuracy: 0.5609 - val_loss: 0.7168 - val_accuracy: 0.4714\n",
      "Epoch 6/50\n",
      "115/115 [==============================] - 3s 28ms/step - loss: 0.6633 - accuracy: 0.6005 - val_loss: 0.6505 - val_accuracy: 0.6327\n",
      "Epoch 7/50\n",
      "115/115 [==============================] - 3s 28ms/step - loss: 0.6465 - accuracy: 0.6311 - val_loss: 0.6184 - val_accuracy: 0.6551\n",
      "Epoch 8/50\n",
      "115/115 [==============================] - 3s 28ms/step - loss: 0.6309 - accuracy: 0.6516 - val_loss: 0.6381 - val_accuracy: 0.6102\n",
      "Epoch 9/50\n",
      "115/115 [==============================] - 3s 28ms/step - loss: 0.6292 - accuracy: 0.6533 - val_loss: 0.6160 - val_accuracy: 0.6592\n",
      "Epoch 10/50\n",
      "115/115 [==============================] - 3s 28ms/step - loss: 0.6149 - accuracy: 0.6604 - val_loss: 0.6031 - val_accuracy: 0.6837\n",
      "Epoch 11/50\n",
      "115/115 [==============================] - 3s 28ms/step - loss: 0.6053 - accuracy: 0.6661 - val_loss: 0.6010 - val_accuracy: 0.6796\n",
      "Epoch 12/50\n",
      "115/115 [==============================] - 3s 28ms/step - loss: 0.6075 - accuracy: 0.6760 - val_loss: 0.5925 - val_accuracy: 0.6776\n",
      "Epoch 13/50\n",
      "115/115 [==============================] - 3s 27ms/step - loss: 0.5937 - accuracy: 0.6891 - val_loss: 0.5931 - val_accuracy: 0.6755\n",
      "Epoch 14/50\n",
      "115/115 [==============================] - 3s 28ms/step - loss: 0.5885 - accuracy: 0.6902 - val_loss: 0.5958 - val_accuracy: 0.6796\n",
      "Epoch 15/50\n",
      "115/115 [==============================] - 3s 28ms/step - loss: 0.5979 - accuracy: 0.6787 - val_loss: 0.5927 - val_accuracy: 0.6735\n",
      "Epoch 16/50\n",
      "115/115 [==============================] - 3s 28ms/step - loss: 0.5842 - accuracy: 0.6883 - val_loss: 0.5890 - val_accuracy: 0.6816\n",
      "Epoch 17/50\n",
      "115/115 [==============================] - 3s 28ms/step - loss: 0.5698 - accuracy: 0.7107 - val_loss: 0.5810 - val_accuracy: 0.6959\n",
      "Epoch 18/50\n",
      "115/115 [==============================] - 3s 28ms/step - loss: 0.5636 - accuracy: 0.7098 - val_loss: 0.5956 - val_accuracy: 0.6918\n",
      "Epoch 19/50\n",
      "115/115 [==============================] - 3s 28ms/step - loss: 0.5561 - accuracy: 0.7292 - val_loss: 0.5821 - val_accuracy: 0.6959\n",
      "Epoch 20/50\n",
      "115/115 [==============================] - 3s 28ms/step - loss: 0.5640 - accuracy: 0.7153 - val_loss: 0.6214 - val_accuracy: 0.6796\n",
      "Epoch 21/50\n",
      "115/115 [==============================] - 3s 28ms/step - loss: 0.5581 - accuracy: 0.7197 - val_loss: 0.5921 - val_accuracy: 0.6878\n",
      "Epoch 22/50\n",
      "115/115 [==============================] - 3s 28ms/step - loss: 0.5402 - accuracy: 0.7393 - val_loss: 0.6036 - val_accuracy: 0.6735\n",
      "Epoch 23/50\n",
      "115/115 [==============================] - 3s 28ms/step - loss: 0.5403 - accuracy: 0.7325 - val_loss: 0.5838 - val_accuracy: 0.7000\n",
      "Epoch 24/50\n",
      "115/115 [==============================] - 3s 28ms/step - loss: 0.5342 - accuracy: 0.7380 - val_loss: 0.6438 - val_accuracy: 0.7061\n",
      "Epoch 25/50\n",
      "115/115 [==============================] - 3s 28ms/step - loss: 0.5205 - accuracy: 0.7470 - val_loss: 0.6319 - val_accuracy: 0.6694\n",
      "Epoch 26/50\n",
      "115/115 [==============================] - 3s 28ms/step - loss: 0.5205 - accuracy: 0.7546 - val_loss: 0.6178 - val_accuracy: 0.7041\n",
      "Epoch 27/50\n",
      "115/115 [==============================] - 3s 28ms/step - loss: 0.5207 - accuracy: 0.7574 - val_loss: 0.6307 - val_accuracy: 0.6510\n",
      "Epoch 28/50\n",
      "115/115 [==============================] - 3s 28ms/step - loss: 0.5000 - accuracy: 0.7743 - val_loss: 0.6131 - val_accuracy: 0.6694\n",
      "Epoch 29/50\n",
      "115/115 [==============================] - 3s 28ms/step - loss: 0.4966 - accuracy: 0.7678 - val_loss: 0.6212 - val_accuracy: 0.6755\n",
      "Epoch 30/50\n",
      "115/115 [==============================] - 3s 28ms/step - loss: 0.4902 - accuracy: 0.7710 - val_loss: 0.6462 - val_accuracy: 0.6735\n",
      "Epoch 31/50\n",
      "115/115 [==============================] - 3s 28ms/step - loss: 0.4665 - accuracy: 0.7926 - val_loss: 0.6498 - val_accuracy: 0.6694\n",
      "Epoch 32/50\n",
      "115/115 [==============================] - 3s 28ms/step - loss: 0.4541 - accuracy: 0.8044 - val_loss: 0.6917 - val_accuracy: 0.6714\n",
      "Epoch 33/50\n",
      "115/115 [==============================] - 3s 28ms/step - loss: 0.4688 - accuracy: 0.7937 - val_loss: 0.6763 - val_accuracy: 0.6551\n",
      "Epoch 34/50\n",
      "115/115 [==============================] - 3s 28ms/step - loss: 0.4433 - accuracy: 0.8112 - val_loss: 0.7092 - val_accuracy: 0.6796\n",
      "Epoch 35/50\n",
      "115/115 [==============================] - 3s 28ms/step - loss: 0.4788 - accuracy: 0.7861 - val_loss: 0.6941 - val_accuracy: 0.6531\n",
      "Epoch 36/50\n",
      "115/115 [==============================] - 3s 28ms/step - loss: 0.4329 - accuracy: 0.8093 - val_loss: 0.6451 - val_accuracy: 0.6612\n",
      "Epoch 37/50\n",
      "115/115 [==============================] - 3s 28ms/step - loss: 0.4166 - accuracy: 0.8232 - val_loss: 0.6802 - val_accuracy: 0.6776\n",
      "Epoch 38/50\n",
      "115/115 [==============================] - 3s 28ms/step - loss: 0.3829 - accuracy: 0.8434 - val_loss: 0.8100 - val_accuracy: 0.6816\n",
      "Epoch 39/50\n",
      "115/115 [==============================] - 3s 28ms/step - loss: 0.3767 - accuracy: 0.8525 - val_loss: 0.7085 - val_accuracy: 0.6673\n",
      "Epoch 40/50\n",
      "115/115 [==============================] - 3s 28ms/step - loss: 0.3681 - accuracy: 0.8541 - val_loss: 0.7606 - val_accuracy: 0.6531\n",
      "Epoch 41/50\n",
      "115/115 [==============================] - 3s 28ms/step - loss: 0.3711 - accuracy: 0.8500 - val_loss: 0.8394 - val_accuracy: 0.6612\n",
      "Epoch 42/50\n",
      "115/115 [==============================] - 3s 28ms/step - loss: 0.3521 - accuracy: 0.8650 - val_loss: 0.7853 - val_accuracy: 0.6735\n",
      "Epoch 43/50\n",
      "115/115 [==============================] - 3s 28ms/step - loss: 0.3421 - accuracy: 0.8656 - val_loss: 0.7096 - val_accuracy: 0.6531\n",
      "Epoch 44/50\n",
      "115/115 [==============================] - 3s 28ms/step - loss: 0.3466 - accuracy: 0.8637 - val_loss: 0.7627 - val_accuracy: 0.6551\n",
      "Epoch 45/50\n",
      "115/115 [==============================] - 3s 28ms/step - loss: 0.3166 - accuracy: 0.8798 - val_loss: 0.8475 - val_accuracy: 0.6592\n",
      "Epoch 46/50\n",
      "115/115 [==============================] - 3s 28ms/step - loss: 0.3144 - accuracy: 0.8847 - val_loss: 0.8094 - val_accuracy: 0.6490\n",
      "Epoch 47/50\n",
      "115/115 [==============================] - 3s 28ms/step - loss: 0.2764 - accuracy: 0.9011 - val_loss: 0.8357 - val_accuracy: 0.6673\n",
      "Epoch 48/50\n",
      "115/115 [==============================] - 3s 28ms/step - loss: 0.2996 - accuracy: 0.8872 - val_loss: 0.8742 - val_accuracy: 0.6327\n",
      "Epoch 49/50\n",
      "115/115 [==============================] - 3s 28ms/step - loss: 0.2810 - accuracy: 0.8978 - val_loss: 0.8642 - val_accuracy: 0.6735\n",
      "Epoch 50/50\n",
      "115/115 [==============================] - 3s 28ms/step - loss: 0.2704 - accuracy: 0.9066 - val_loss: 0.8139 - val_accuracy: 0.6449\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f19bd86ae50>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_epochs = 50\n",
    "MLmodel.fit(X_train_tensor, Y_train_tensor, epochs=num_epochs, validation_data=(X_test_tensor, Y_test_tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a5de6248-5092-46ef-9b21-6b0530841554",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-04 01:51:58.288826: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-05-04 01:51:58.290678: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-05-04 01:51:58.292227: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-05-04 01:51:58.444283: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bidirectional (Bidirectiona  (None, 300)              541200    \n",
      " l)                                                              \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 300)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 16)                4816      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 16)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 2)                 34        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 546,050\n",
      "Trainable params: 546,050\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-04 01:51:58.497821: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-05-04 01:51:58.499494: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-05-04 01:51:58.500747: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(150)),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    tf.keras.layers.Dense(16, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    tf.keras.layers.Dense(2, activation='softmax')\n",
    "])\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.build(input_shape=(None, 43, 300))\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "48433f6b-afbf-4f61-8572-231991bd38b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-04 01:52:02.195792: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-05-04 01:52:02.197650: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-05-04 01:52:02.198982: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-05-04 01:52:02.361996: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis}}]]\n",
      "2023-05-04 01:52:02.420955: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-05-04 01:52:02.422678: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-05-04 01:52:02.424050: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-05-04 01:52:02.992886: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis}}]]\n",
      "2023-05-04 01:52:03.496436: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-05-04 01:52:03.498158: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-05-04 01:52:03.500412: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-05-04 01:52:03.659800: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis}}]]\n",
      "2023-05-04 01:52:03.718203: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-05-04 01:52:03.719966: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-05-04 01:52:03.721342: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-05-04 01:52:04.285583: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115/115 [==============================] - ETA: 0s - loss: 0.6911 - accuracy: 0.5393"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-04 01:52:10.110350: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-05-04 01:52:10.111868: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-05-04 01:52:10.113163: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-05-04 01:52:10.266370: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis}}]]\n",
      "2023-05-04 01:52:10.321522: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-05-04 01:52:10.323014: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-05-04 01:52:10.324247: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115/115 [==============================] - 9s 49ms/step - loss: 0.6911 - accuracy: 0.5393 - val_loss: 0.6816 - val_accuracy: 0.5837\n",
      "Epoch 2/50\n",
      "115/115 [==============================] - 5s 40ms/step - loss: 0.6658 - accuracy: 0.5926 - val_loss: 0.6514 - val_accuracy: 0.6327\n",
      "Epoch 3/50\n",
      "115/115 [==============================] - 5s 41ms/step - loss: 0.6540 - accuracy: 0.6172 - val_loss: 0.6513 - val_accuracy: 0.6020\n",
      "Epoch 4/50\n",
      "115/115 [==============================] - 5s 42ms/step - loss: 0.6352 - accuracy: 0.6380 - val_loss: 0.6140 - val_accuracy: 0.6571\n",
      "Epoch 5/50\n",
      "115/115 [==============================] - 5s 41ms/step - loss: 0.6227 - accuracy: 0.6489 - val_loss: 0.6588 - val_accuracy: 0.6490\n",
      "Epoch 6/50\n",
      "115/115 [==============================] - 5s 42ms/step - loss: 0.6135 - accuracy: 0.6661 - val_loss: 0.6068 - val_accuracy: 0.6531\n",
      "Epoch 7/50\n",
      "115/115 [==============================] - 5s 40ms/step - loss: 0.5996 - accuracy: 0.6792 - val_loss: 0.6430 - val_accuracy: 0.6429\n",
      "Epoch 8/50\n",
      "115/115 [==============================] - 5s 42ms/step - loss: 0.5934 - accuracy: 0.6825 - val_loss: 0.5988 - val_accuracy: 0.6571\n",
      "Epoch 9/50\n",
      "115/115 [==============================] - 5s 41ms/step - loss: 0.5836 - accuracy: 0.6910 - val_loss: 0.5915 - val_accuracy: 0.6735\n",
      "Epoch 10/50\n",
      "115/115 [==============================] - 5s 41ms/step - loss: 0.5702 - accuracy: 0.7057 - val_loss: 0.6103 - val_accuracy: 0.6510\n",
      "Epoch 11/50\n",
      "115/115 [==============================] - 4s 39ms/step - loss: 0.5647 - accuracy: 0.7142 - val_loss: 0.5980 - val_accuracy: 0.6653\n",
      "Epoch 12/50\n",
      "115/115 [==============================] - 4s 39ms/step - loss: 0.5491 - accuracy: 0.7224 - val_loss: 0.6605 - val_accuracy: 0.6367\n",
      "Epoch 13/50\n",
      "115/115 [==============================] - 4s 39ms/step - loss: 0.5308 - accuracy: 0.7361 - val_loss: 0.6645 - val_accuracy: 0.6612\n",
      "Epoch 14/50\n",
      "115/115 [==============================] - 4s 39ms/step - loss: 0.5175 - accuracy: 0.7495 - val_loss: 0.6445 - val_accuracy: 0.6449\n",
      "Epoch 15/50\n",
      "115/115 [==============================] - 4s 39ms/step - loss: 0.5147 - accuracy: 0.7451 - val_loss: 0.6331 - val_accuracy: 0.6694\n",
      "Epoch 16/50\n",
      "115/115 [==============================] - 4s 39ms/step - loss: 0.4848 - accuracy: 0.7661 - val_loss: 0.6376 - val_accuracy: 0.6571\n",
      "Epoch 17/50\n",
      "115/115 [==============================] - 4s 39ms/step - loss: 0.4700 - accuracy: 0.7814 - val_loss: 0.7544 - val_accuracy: 0.6265\n",
      "Epoch 18/50\n",
      "115/115 [==============================] - 4s 39ms/step - loss: 0.4554 - accuracy: 0.7844 - val_loss: 0.7603 - val_accuracy: 0.6510\n",
      "Epoch 19/50\n",
      "115/115 [==============================] - 4s 39ms/step - loss: 0.4196 - accuracy: 0.8090 - val_loss: 0.7391 - val_accuracy: 0.6531\n",
      "Epoch 20/50\n",
      "115/115 [==============================] - 5s 39ms/step - loss: 0.4068 - accuracy: 0.8178 - val_loss: 0.7235 - val_accuracy: 0.6510\n",
      "Epoch 21/50\n",
      "115/115 [==============================] - 4s 39ms/step - loss: 0.3795 - accuracy: 0.8377 - val_loss: 0.7516 - val_accuracy: 0.6592\n",
      "Epoch 22/50\n",
      "115/115 [==============================] - 5s 41ms/step - loss: 0.3465 - accuracy: 0.8475 - val_loss: 0.8340 - val_accuracy: 0.6551\n",
      "Epoch 23/50\n",
      "115/115 [==============================] - 5s 40ms/step - loss: 0.3240 - accuracy: 0.8609 - val_loss: 0.9341 - val_accuracy: 0.6612\n",
      "Epoch 24/50\n",
      "115/115 [==============================] - 4s 39ms/step - loss: 0.3213 - accuracy: 0.8637 - val_loss: 0.9712 - val_accuracy: 0.6429\n",
      "Epoch 25/50\n",
      "115/115 [==============================] - 4s 39ms/step - loss: 0.2898 - accuracy: 0.8809 - val_loss: 1.0948 - val_accuracy: 0.6714\n",
      "Epoch 26/50\n",
      "115/115 [==============================] - 5s 39ms/step - loss: 0.2779 - accuracy: 0.8825 - val_loss: 0.9917 - val_accuracy: 0.6510\n",
      "Epoch 27/50\n",
      "115/115 [==============================] - 4s 39ms/step - loss: 0.2578 - accuracy: 0.8943 - val_loss: 1.0859 - val_accuracy: 0.6551\n",
      "Epoch 28/50\n",
      "115/115 [==============================] - 4s 39ms/step - loss: 0.2273 - accuracy: 0.9060 - val_loss: 1.1782 - val_accuracy: 0.6490\n",
      "Epoch 29/50\n",
      "115/115 [==============================] - 4s 39ms/step - loss: 0.1985 - accuracy: 0.9178 - val_loss: 1.2884 - val_accuracy: 0.6408\n",
      "Epoch 30/50\n",
      "115/115 [==============================] - 4s 39ms/step - loss: 0.1825 - accuracy: 0.9273 - val_loss: 1.2627 - val_accuracy: 0.6347\n",
      "Epoch 31/50\n",
      "115/115 [==============================] - 4s 39ms/step - loss: 0.1940 - accuracy: 0.9216 - val_loss: 1.3202 - val_accuracy: 0.6306\n",
      "Epoch 32/50\n",
      "115/115 [==============================] - 4s 39ms/step - loss: 0.1759 - accuracy: 0.9320 - val_loss: 1.3038 - val_accuracy: 0.6449\n",
      "Epoch 33/50\n",
      "115/115 [==============================] - 4s 39ms/step - loss: 0.1297 - accuracy: 0.9516 - val_loss: 1.7952 - val_accuracy: 0.6245\n",
      "Epoch 34/50\n",
      "115/115 [==============================] - 4s 39ms/step - loss: 0.1467 - accuracy: 0.9423 - val_loss: 1.5964 - val_accuracy: 0.6224\n",
      "Epoch 35/50\n",
      "115/115 [==============================] - 4s 39ms/step - loss: 0.1319 - accuracy: 0.9497 - val_loss: 1.9036 - val_accuracy: 0.6020\n",
      "Epoch 36/50\n",
      "115/115 [==============================] - 4s 39ms/step - loss: 0.1225 - accuracy: 0.9538 - val_loss: 1.6395 - val_accuracy: 0.6143\n",
      "Epoch 37/50\n",
      "115/115 [==============================] - 4s 39ms/step - loss: 0.0969 - accuracy: 0.9639 - val_loss: 1.6571 - val_accuracy: 0.6388\n",
      "Epoch 38/50\n",
      "115/115 [==============================] - 5s 39ms/step - loss: 0.0915 - accuracy: 0.9658 - val_loss: 1.7618 - val_accuracy: 0.6265\n",
      "Epoch 39/50\n",
      "115/115 [==============================] - 4s 39ms/step - loss: 0.1050 - accuracy: 0.9604 - val_loss: 1.8737 - val_accuracy: 0.6347\n",
      "Epoch 40/50\n",
      "115/115 [==============================] - 4s 39ms/step - loss: 0.0913 - accuracy: 0.9678 - val_loss: 1.9957 - val_accuracy: 0.6510\n",
      "Epoch 41/50\n",
      "115/115 [==============================] - 4s 39ms/step - loss: 0.0790 - accuracy: 0.9708 - val_loss: 2.0622 - val_accuracy: 0.6408\n",
      "Epoch 42/50\n",
      "115/115 [==============================] - 4s 39ms/step - loss: 0.0811 - accuracy: 0.9754 - val_loss: 2.0123 - val_accuracy: 0.6408\n",
      "Epoch 43/50\n",
      "115/115 [==============================] - 4s 39ms/step - loss: 0.0780 - accuracy: 0.9730 - val_loss: 2.2089 - val_accuracy: 0.6245\n",
      "Epoch 44/50\n",
      "115/115 [==============================] - 4s 39ms/step - loss: 0.0874 - accuracy: 0.9705 - val_loss: 2.0631 - val_accuracy: 0.6224\n",
      "Epoch 45/50\n",
      "115/115 [==============================] - 5s 39ms/step - loss: 0.0686 - accuracy: 0.9746 - val_loss: 2.2788 - val_accuracy: 0.6367\n",
      "Epoch 46/50\n",
      "115/115 [==============================] - 4s 39ms/step - loss: 0.0722 - accuracy: 0.9746 - val_loss: 1.8251 - val_accuracy: 0.6347\n",
      "Epoch 47/50\n",
      "115/115 [==============================] - 5s 41ms/step - loss: 0.0519 - accuracy: 0.9828 - val_loss: 2.1710 - val_accuracy: 0.6531\n",
      "Epoch 48/50\n",
      "115/115 [==============================] - 5s 42ms/step - loss: 0.0634 - accuracy: 0.9779 - val_loss: 2.1842 - val_accuracy: 0.6082\n",
      "Epoch 49/50\n",
      "115/115 [==============================] - 5s 43ms/step - loss: 0.0563 - accuracy: 0.9831 - val_loss: 2.1569 - val_accuracy: 0.6429\n",
      "Epoch 50/50\n",
      "115/115 [==============================] - 5s 41ms/step - loss: 0.0341 - accuracy: 0.9885 - val_loss: 2.5067 - val_accuracy: 0.6449\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f18e6772fd0>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " num_epochs = 50\n",
    "model.fit(X_train_tensor, Y_train_tensor, epochs=num_epochs, validation_data=(X_test_tensor, Y_test_tensor))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "724fc8ae-5e16-41d9-ac56-985cf6d17e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Emoticons\n",
    "emoticons = \\\n",
    "\t[\t# For __EMOT_SMILEY\n",
    "        (' __emoji: U+1F601',\t[':-)', ':)', '(:', '(-:', ] )\t,\\\n",
    "        # for __EMOT_LAUGH\n",
    "\t\t(' __emoji: U+1F923',\t\t[':-D', ':D', 'X-D', 'XD', 'xD', ] )\t,\\\n",
    "        # For __EMOT_LOVE\n",
    "\t\t(' __emoji: U+2764',\t\t['<3', ':\\*', ] )\t,\\\n",
    "        # For __EMOT_WINK\n",
    "\t\t('__emoji: U+1F609',\t\t[';-)', ';)', ';-D', ';D', '(;', '(-;', ] )\t,\\\n",
    "        # For __EMOT_FROWN\n",
    "\t\t(' __emoji: U+2639',\t\t[':-(', ':(', '(:', '(-:', ] )\t,\\\n",
    "        # For __EMOT_CRY\n",
    "\t\t(' __emoji: U+1F622',\t\t[':,(', ':\\'(', ':\"(', ':(('] )\t,\\\n",
    "\t]\n",
    "    \n",
    "def escape_paren(arr):\n",
    "\treturn [text.replace(')', '[)}\\]]').replace('(', '[({\\[]') for text in arr]\n",
    "\n",
    "def regex_union(arr):\n",
    "\treturn '(' + '|'.join( arr ) + ')'\n",
    "\n",
    "emoticons_regex = [ (repl, re.compile(regex_union(escape_paren(regx))) ) for (repl, regx) in emoticons ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f6adbb8d-116d-4f12-93ed-23967d97ae7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a text with one emoticon   __emoji: U+1F601  and another   __emoji: U+2639 \n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "text = \"This is a text with one emoticon :) and another :(\"\n",
    "for (repl, regx) in emoticons_regex :\n",
    "    text = re.sub(regx, ' '+repl+' ', text)\n",
    "    \n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "efe32632-849a-48b3-87d5-123337b37f44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emoji_category</th>\n",
       "      <th>number</th>\n",
       "      <th>code</th>\n",
       "      <th>CLDR_Short_Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>face-positive</td>\n",
       "      <td>1</td>\n",
       "      <td>U+1F600</td>\n",
       "      <td>grinning face</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>face-positive</td>\n",
       "      <td>2</td>\n",
       "      <td>U+1F601</td>\n",
       "      <td>beaming face with smiling eyes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>face-positive</td>\n",
       "      <td>3</td>\n",
       "      <td>U+1F602</td>\n",
       "      <td>face with tears of joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>face-positive</td>\n",
       "      <td>4</td>\n",
       "      <td>U+1F923</td>\n",
       "      <td>rolling on the floor laughing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>face-positive</td>\n",
       "      <td>5</td>\n",
       "      <td>U+1F603</td>\n",
       "      <td>grinning face with big eyes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  emoji_category  number     code                 CLDR_Short_Name\n",
       "0  face-positive       1  U+1F600                   grinning face\n",
       "1  face-positive       2  U+1F601  beaming face with smiling eyes\n",
       "2  face-positive       3  U+1F602          face with tears of joy\n",
       "3  face-positive       4  U+1F923   rolling on the floor laughing\n",
       "4  face-positive       5  U+1F603     grinning face with big eyes"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emojis_db=pd.read_csv('training/emojis_db_csv.csv')\n",
    "emojis_db.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4bd6247a-2b74-4814-87e3-98615a41c620",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['face-positive', 'face-neutral', 'face-negative', 'face-sick',\n",
       "       'face-role', 'face-fantasy', 'cat-face', 'monkey-face', 'person',\n",
       "       'person-role', 'person-fantasy', 'person-gesture',\n",
       "       'person-activity', 'person-sport', 'family', 'body', 'hair-style',\n",
       "       'emotion', 'clothing', 'animal-mammal', 'animal-bird',\n",
       "       'animal-amphibian', 'animal-reptile', 'animal-marine',\n",
       "       'animal-bug', 'plant-flower', 'plant-other', 'food-fruit',\n",
       "       'food-vegetable', 'food-prepared', 'food-asian', 'food-sweet',\n",
       "       'drink', 'dishware', 'place-map', 'place-geographic',\n",
       "       'place-building', 'place-religious', 'place-other',\n",
       "       'transport-ground', 'transport-water', 'transport-air', 'hotel',\n",
       "       'time', 'sky & weather', 'event', 'award-medal', 'sport', 'game',\n",
       "       'arts & crafts', 'sound', 'music', 'musical-instrument', 'phone',\n",
       "       'computer', 'light & video', 'book-paper', 'money', 'mail',\n",
       "       'writing', 'office', 'lock', 'tool', 'science', 'medical',\n",
       "       'household', 'other-object', 'transport-sign', 'warning', 'arrow',\n",
       "       'religion', 'zodiac', 'av-symbol', 'other-symbol', 'keycap',\n",
       "       'alphanum', 'geometric', 'flag', 'country-flag',\n",
       "       'subdivision-flag'], dtype=object)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emojis_db.emoji_category.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "32954ed9-3129-4537-a9d3-4f93579527e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def emojis_unicode(tweet):\n",
    "    ''' Extracts the emojis on the tweet on Unicode format, also tries to match those in regular format, such as \";)\" '''\n",
    "    for (repl, regx) in emoticons_regex :\n",
    "        tweet = re.sub(regx, ' '+repl+' ', tweet)\n",
    "#     print(tweet)\n",
    "\n",
    "    tweet_unicode = str(tweet.encode('unicode-escape'))\n",
    "    tweet_unicode = tweet_unicode.replace('\\\\\\\\U000',' __emoji: U+')\n",
    "#     print(tweet_unicode)\n",
    "    \n",
    "    emoji_list = []\n",
    "#     print(tweet)\n",
    "    for emoji in range(tweet_unicode.count(' __emoji: ')):\n",
    "        em = tweet_unicode.split('__emoji: ')[emoji+1].split()[0]\n",
    "        em = em[:7] # the len of the emoji in unicode is between 6 and 7\n",
    "        emoji_list.append(em.upper())\n",
    "    return emoji_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "29e25da8-3d7c-4566-aa72-12da726855f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['U+1F601', 'U+2639', 'U+1F621', 'U+1F923', 'U+1F602']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"This is a text with one emoticon :) and another :( and some others: 😡 🤣😂\"\n",
    "emojis_unicode(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e5c1be08-86e4-41d2-a624-10fc92a48860",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['face-positive',\n",
       " 'face-negative',\n",
       " 'face-negative',\n",
       " 'face-positive',\n",
       " 'face-positive']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def emoji_category(emojis):\n",
    "    categories = []\n",
    "    for i in range(len(emojis)):\n",
    "        # print(emojis[i])\n",
    "        try:\n",
    "            categories.append(emojis_db.loc[emojis_db.code == emojis[i]].emoji_category.values[0])\n",
    "        except:\n",
    "            try:\n",
    "                _ = emojis[i].split('+')[1] + '+'\n",
    "                categories.append(emojis_db[emojis_db.code.str.contains(_)].emoji_category.values[0])\n",
    "            except:\n",
    "                categories.append('other')\n",
    "    if len(categories) < 1:\n",
    "        categories.append('no_emojis')\n",
    "    return categories\n",
    "\n",
    "emoji_category(emojis_unicode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4f5646c9-9e50-4957-823b-090709c0682c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unigrams\n",
    "unigrams_fd = nltk.FreqDist()\n",
    "# unigrams_fd.update(text)\n",
    "# unigrams_fd\n",
    "\n",
    "# Bigrams\n",
    "# words_bi  = [ ','.join(map(str,bg)) for bg in nltk.bigrams(text) ]\n",
    "bi_grams_fd = nltk.FreqDist()\n",
    "# bi_grams_fd.update( words_bi )\n",
    "# bi_grams_fd\n",
    "\n",
    "# Trigrams\n",
    "# words_tri  = [ ','.join(map(str,tg)) for tg in nltk.trigrams(text) ]\n",
    "tri_grams_fd = nltk.FreqDist()\n",
    "# tri_grams_fd.update( words_tri )\n",
    "# tri_grams_fd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1141dd94-5e8c-40e5-af3e-7acbd4f349f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapper function that encloses all the n-grams procedures\n",
    "def get_word_features(words):\n",
    "    bag = {}\n",
    "    words_uni = [ 'has(%s)'% ug for ug in words ]\n",
    "    words_bi  = [ 'has(%s)'% ','.join(map(str,bg)) for bg in nltk.bigrams(words) ]\n",
    "    words_tri = [ 'has(%s)'% ','.join(map(str,tg)) for tg in nltk.trigrams(words) ]\n",
    "    \n",
    "    for f in words_uni+words_bi+words_tri:\n",
    "        bag[f] = 1\n",
    "\n",
    "    return bag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "026567db-d16c-419b-8bb7-e515cc354ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(text):\n",
    "    global usr_names, hstgs\n",
    "    features = {}\n",
    "    words = text\n",
    "    words = processAll(text)\n",
    "\n",
    "    word_features = get_word_features(words)\n",
    "    features.update( word_features )\n",
    "\n",
    "    negation_features = get_negation_features(words)\n",
    "    features.update( negation_features )\n",
    "    \n",
    "    # Sentiment features are not included on the final deliverabe as did not improve results\n",
    "#     sentiment_features = get_polarity_features(text)\n",
    "#     features.update(sentiment_features )\n",
    "    \n",
    "    emoji_features = emoji_category(emojis_unicode(text))\n",
    "    emoji_features_dic = dict( zip(['emoji_('+w+')' for w in  emoji_features], emoji_features))\n",
    "    features.update( emoji_features_dic )\n",
    "    \n",
    "    usr_names = list(set(usr_names))\n",
    "    hstgs = list(set(hstgs))\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "30a89f26-d8c0-4236-af13-2ef3c39021f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "negtn_regex = re.compile( r\"\"\"(?:\n",
    "    ^(?:nunca|no|nada|ningún|ninguno|ninguna|tampoco|\n",
    "        nunc|nad|ningun|tampoc\n",
    "    )$\n",
    ")\n",
    "|\n",
    "n't\n",
    "\"\"\", re.X)\n",
    "\n",
    "def get_negation_features(words):\n",
    "    INF = 0.0\n",
    "    negtn = [ bool(negtn_regex.search(w)) for w in words ]\n",
    "\n",
    "    left = [0.0] * len(words)\n",
    "    prev = 0.0\n",
    "    for i in range(0,len(words)):\n",
    "        if( negtn[i] ):\n",
    "            prev = 1.0\n",
    "        left[i] = prev\n",
    "        prev = max( 0.0, prev-0.1)\n",
    "\n",
    "    right = [0.0] * len(words)\n",
    "    prev = 0.0\n",
    "    for i in reversed(range(0,len(words))):\n",
    "        if( negtn[i] ):\n",
    "            prev = 1.0\n",
    "        right[i] = prev\n",
    "        prev = max( 0.0, prev-0.1)\n",
    "\n",
    "    return dict( zip(\n",
    "                    ['neg_l('+w+')' for w in  words] + ['neg_r('+w+')' for w in  words],\n",
    "                    left + right ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7d1e703f-f8f2-46cd-9c75-60ee4539e853",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['processed_tweet_features'] = train.tweet.apply(extract_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "df13f383-6165-4a2b-bc8d-b0b2111695cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_EXIST</th>\n",
       "      <th>lang</th>\n",
       "      <th>tweet</th>\n",
       "      <th>number_annotators</th>\n",
       "      <th>annotators</th>\n",
       "      <th>gender_annotators</th>\n",
       "      <th>age_annotators</th>\n",
       "      <th>labels_task1</th>\n",
       "      <th>labels_task2</th>\n",
       "      <th>labels_task3</th>\n",
       "      <th>split</th>\n",
       "      <th>label1</th>\n",
       "      <th>processed_tweet</th>\n",
       "      <th>processed_tweet_features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>100001</th>\n",
       "      <td>100001</td>\n",
       "      <td>es</td>\n",
       "      <td>@TheChiflis Ignora al otro, es un capullo.El p...</td>\n",
       "      <td>6</td>\n",
       "      <td>[Annotator_1, Annotator_2, Annotator_3, Annota...</td>\n",
       "      <td>[F, F, F, M, M, M]</td>\n",
       "      <td>[18-22, 23-45, 46+, 46+, 23-45, 18-22]</td>\n",
       "      <td>[YES, YES, NO, YES, YES, YES]</td>\n",
       "      <td>[REPORTED, JUDGEMENTAL, -, REPORTED, JUDGEMENT...</td>\n",
       "      <td>[[OBJECTIFICATION], [OBJECTIFICATION, SEXUAL-V...</td>\n",
       "      <td>TRAIN_ES</td>\n",
       "      <td>YES</td>\n",
       "      <td>[__user_THECHIFLIS, ignor, otro, capull, probl...</td>\n",
       "      <td>{'has(__user_THECHIFLIS)': 1, 'has(ignor)': 1,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100005</th>\n",
       "      <td>100005</td>\n",
       "      <td>es</td>\n",
       "      <td>@novadragon21 @icep4ck @TvDannyZ Entonces como...</td>\n",
       "      <td>6</td>\n",
       "      <td>[Annotator_19, Annotator_20, Annotator_21, Ann...</td>\n",
       "      <td>[F, F, F, M, M, M]</td>\n",
       "      <td>[18-22, 23-45, 46+, 46+, 23-45, 18-22]</td>\n",
       "      <td>[YES, NO, YES, NO, YES, YES]</td>\n",
       "      <td>[REPORTED, -, JUDGEMENTAL, -, JUDGEMENTAL, DIR...</td>\n",
       "      <td>[[STEREOTYPING-DOMINANCE, OBJECTIFICATION], [-...</td>\n",
       "      <td>TRAIN_ES</td>\n",
       "      <td>YES</td>\n",
       "      <td>[__user_NOVADRAGON21, __user_ICEP4CK, __user_T...</td>\n",
       "      <td>{'has(__user_NOVADRAGON21)': 1, 'has(__user_IC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100008</th>\n",
       "      <td>100008</td>\n",
       "      <td>es</td>\n",
       "      <td>@BestKabest Esta gringa sigue llorando por el ...</td>\n",
       "      <td>6</td>\n",
       "      <td>[Annotator_25, Annotator_26, Annotator_27, Ann...</td>\n",
       "      <td>[F, F, F, M, M, M]</td>\n",
       "      <td>[18-22, 23-45, 46+, 46+, 23-45, 18-22]</td>\n",
       "      <td>[YES, YES, YES, YES, YES, YES]</td>\n",
       "      <td>[DIRECT, DIRECT, DIRECT, JUDGEMENTAL, DIRECT, ...</td>\n",
       "      <td>[[IDEOLOGICAL-INEQUALITY], [STEREOTYPING-DOMIN...</td>\n",
       "      <td>TRAIN_ES</td>\n",
       "      <td>YES</td>\n",
       "      <td>[__user_BESTKABEST, esta, gring, sig, llor, po...</td>\n",
       "      <td>{'has(__user_BESTKABEST)': 1, 'has(esta)': 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100028</th>\n",
       "      <td>100028</td>\n",
       "      <td>es</td>\n",
       "      <td>@ShahidForChange @TeamPelosi Quiet, sexist ^%$...</td>\n",
       "      <td>6</td>\n",
       "      <td>[Annotator_109, Annotator_110, Annotator_111, ...</td>\n",
       "      <td>[F, F, F, M, M, M]</td>\n",
       "      <td>[18-22, 23-45, 46+, 46+, 23-45, 18-22]</td>\n",
       "      <td>[YES, NO, NO, YES, YES, YES]</td>\n",
       "      <td>[JUDGEMENTAL, -, -, DIRECT, DIRECT, DIRECT]</td>\n",
       "      <td>[[STEREOTYPING-DOMINANCE], [-], [-], [IDEOLOGI...</td>\n",
       "      <td>TRAIN_ES</td>\n",
       "      <td>YES</td>\n",
       "      <td>[__user_SHAHIDFORCHANGE, __user_TEAMPELOSI, qu...</td>\n",
       "      <td>{'has(__user_SHAHIDFORCHANGE)': 1, 'has(__user...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100036</th>\n",
       "      <td>100036</td>\n",
       "      <td>es</td>\n",
       "      <td>@Harassed_girl loca d mierda en k momento</td>\n",
       "      <td>6</td>\n",
       "      <td>[Annotator_115, Annotator_116, Annotator_117, ...</td>\n",
       "      <td>[F, F, F, M, M, M]</td>\n",
       "      <td>[18-22, 23-45, 46+, 46+, 23-45, 18-22]</td>\n",
       "      <td>[YES, YES, YES, YES, YES, NO]</td>\n",
       "      <td>[DIRECT, DIRECT, DIRECT, DIRECT, DIRECT, -]</td>\n",
       "      <td>[[STEREOTYPING-DOMINANCE, SEXUAL-VIOLENCE], [M...</td>\n",
       "      <td>TRAIN_ES</td>\n",
       "      <td>YES</td>\n",
       "      <td>[__user_HARASSED_GIRL, loc, mierd, moment]</td>\n",
       "      <td>{'has(__user_HARASSED_GIRL)': 1, 'has(loc)': 1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id_EXIST lang                                              tweet  \\\n",
       "100001   100001   es  @TheChiflis Ignora al otro, es un capullo.El p...   \n",
       "100005   100005   es  @novadragon21 @icep4ck @TvDannyZ Entonces como...   \n",
       "100008   100008   es  @BestKabest Esta gringa sigue llorando por el ...   \n",
       "100028   100028   es  @ShahidForChange @TeamPelosi Quiet, sexist ^%$...   \n",
       "100036   100036   es          @Harassed_girl loca d mierda en k momento   \n",
       "\n",
       "       number_annotators                                         annotators  \\\n",
       "100001                 6  [Annotator_1, Annotator_2, Annotator_3, Annota...   \n",
       "100005                 6  [Annotator_19, Annotator_20, Annotator_21, Ann...   \n",
       "100008                 6  [Annotator_25, Annotator_26, Annotator_27, Ann...   \n",
       "100028                 6  [Annotator_109, Annotator_110, Annotator_111, ...   \n",
       "100036                 6  [Annotator_115, Annotator_116, Annotator_117, ...   \n",
       "\n",
       "         gender_annotators                          age_annotators  \\\n",
       "100001  [F, F, F, M, M, M]  [18-22, 23-45, 46+, 46+, 23-45, 18-22]   \n",
       "100005  [F, F, F, M, M, M]  [18-22, 23-45, 46+, 46+, 23-45, 18-22]   \n",
       "100008  [F, F, F, M, M, M]  [18-22, 23-45, 46+, 46+, 23-45, 18-22]   \n",
       "100028  [F, F, F, M, M, M]  [18-22, 23-45, 46+, 46+, 23-45, 18-22]   \n",
       "100036  [F, F, F, M, M, M]  [18-22, 23-45, 46+, 46+, 23-45, 18-22]   \n",
       "\n",
       "                          labels_task1  \\\n",
       "100001   [YES, YES, NO, YES, YES, YES]   \n",
       "100005    [YES, NO, YES, NO, YES, YES]   \n",
       "100008  [YES, YES, YES, YES, YES, YES]   \n",
       "100028    [YES, NO, NO, YES, YES, YES]   \n",
       "100036   [YES, YES, YES, YES, YES, NO]   \n",
       "\n",
       "                                             labels_task2  \\\n",
       "100001  [REPORTED, JUDGEMENTAL, -, REPORTED, JUDGEMENT...   \n",
       "100005  [REPORTED, -, JUDGEMENTAL, -, JUDGEMENTAL, DIR...   \n",
       "100008  [DIRECT, DIRECT, DIRECT, JUDGEMENTAL, DIRECT, ...   \n",
       "100028        [JUDGEMENTAL, -, -, DIRECT, DIRECT, DIRECT]   \n",
       "100036        [DIRECT, DIRECT, DIRECT, DIRECT, DIRECT, -]   \n",
       "\n",
       "                                             labels_task3     split label1  \\\n",
       "100001  [[OBJECTIFICATION], [OBJECTIFICATION, SEXUAL-V...  TRAIN_ES    YES   \n",
       "100005  [[STEREOTYPING-DOMINANCE, OBJECTIFICATION], [-...  TRAIN_ES    YES   \n",
       "100008  [[IDEOLOGICAL-INEQUALITY], [STEREOTYPING-DOMIN...  TRAIN_ES    YES   \n",
       "100028  [[STEREOTYPING-DOMINANCE], [-], [-], [IDEOLOGI...  TRAIN_ES    YES   \n",
       "100036  [[STEREOTYPING-DOMINANCE, SEXUAL-VIOLENCE], [M...  TRAIN_ES    YES   \n",
       "\n",
       "                                          processed_tweet  \\\n",
       "100001  [__user_THECHIFLIS, ignor, otro, capull, probl...   \n",
       "100005  [__user_NOVADRAGON21, __user_ICEP4CK, __user_T...   \n",
       "100008  [__user_BESTKABEST, esta, gring, sig, llor, po...   \n",
       "100028  [__user_SHAHIDFORCHANGE, __user_TEAMPELOSI, qu...   \n",
       "100036         [__user_HARASSED_GIRL, loc, mierd, moment]   \n",
       "\n",
       "                                 processed_tweet_features  \n",
       "100001  {'has(__user_THECHIFLIS)': 1, 'has(ignor)': 1,...  \n",
       "100005  {'has(__user_NOVADRAGON21)': 1, 'has(__user_IC...  \n",
       "100008  {'has(__user_BESTKABEST)': 1, 'has(esta)': 1, ...  \n",
       "100028  {'has(__user_SHAHIDFORCHANGE)': 1, 'has(__user...  \n",
       "100036  {'has(__user_HARASSED_GIRL)': 1, 'has(loc)': 1...  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e61d1ce-5a70-4b5c-9a1b-b81bc50988b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(train_.shape[0]*0.8)\n",
    "sentiment_train_tweets = [(tweet, sentiment) for tweet, sentiment in train_[['tweetText', 'polarity_value']].values[:train_size]]\n",
    "sentiment_train_tweets_full = [(tweet, sentiment) for tweet, sentiment in train_[['tweetText', 'polarity_value']].values]\n",
    "sentiment_validation_tweets = [(tweet, sentiment) for tweet, sentiment in train_[['tweetText', 'polarity_value']].values[train_size:]]\n",
    "sentiment_test_tweets  = [(tweet, sentiment) for tweet, sentiment in test_[['tweetText', 'polarity_value']].values]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
